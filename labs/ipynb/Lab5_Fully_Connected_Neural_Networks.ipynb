{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vxv_saeDbyUH"
   },
   "source": [
    "# **Lab 5: Fully Connected Neural Networks**\n",
    "\n",
    "---\n",
    "NOTE: This is a lab project accompanying the following book [MLF] and it should be used together with the book.\n",
    "\n",
    "[MLF] *H. Jiang*, \"[Machine Learning Fundamentals: A Concise Introduction](http://wiki.eecs.yorku.ca/user/hj/research:mlfbook)\", Cambridge University Press, 2021.  ([bibtex](http://www.cse.yorku.ca/~hj/mlf-jiang.bib))\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sC6-DExLb8L7"
   },
   "source": [
    "The purpose of this lab is to study various methods to implement fully-connected neural networks (FCNNs) for pattern classification problems. First, we show how to use the MLP implementation from *scikit-learn* to build FCNN classifiers. Second, we will show how to use *numpy* to implement FCNNs from scratch based on the derived formula to compute both forward and backward passes. As training processes of large neural networks are typically very time-consuming on CPUs, furthermore, we will show how to use *JAX* to replace all *numpy* functions so as to run on GPUs. By doing so, we can see that the training speed of neural networks can be signicantly accelerated. Finally, we will show how to use the automatic differentiation (AD) functions available in *JAX* to re-implement FCNNs. In this case, we do not need to worry about how to compute gradients in our program because *jax.grad* will automatically derive gradients according to an objective function that is defined based on the model structure and learning criterion. As we can see, this can dramatically reduce our development efforts in implementing many machine learning models since we only need to define the forward pass of a model, and the model can be learned based on the automatically derived gradients from *jax.grad*.\n",
    "\n",
    "\n",
    "Prerequisites: basic understanding on *JAX*, including *jax.numpy* and *jax.grad*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNdLgww8cJ2F"
   },
   "source": [
    "## **I. Neural networks from scikit-learn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjk6HU6Ortvv"
   },
   "source": [
    "### **Example 5.1:**\n",
    "\n",
    "*Use the neural network implementation from scikit-learn, i.e. MLPClassifier, to build a classifier to recognize all ten digits in the MNIST data set. Investigate the best architecture for the fully connected neural networks (i.e., number of hidden layers and number of hidden nodes per layer) for this task, and fine-tune all hyper-parameters in the SGD optimizer towards the best possible performance.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KH0kWSk_pywk",
    "outputId": "7d572a5f-47b2-4d4f-c2cc-2ce2d64560c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#link my Google drive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fupbz3a-p2zy",
    "outputId": "b0215671-ad97-4e28-bcc8-135e20af930c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python_mnist\n",
      "  Downloading python_mnist-0.7-py2.py3-none-any.whl (9.6 kB)\n",
      "Installing collected packages: python-mnist\n",
      "Successfully installed python-mnist-0.7\n"
     ]
    }
   ],
   "source": [
    "# install python_mnist\n",
    "\n",
    "!pip install python_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tdwEWWClp5l4",
    "outputId": "064310ee-f258-4896-887a-cfc8e758f125"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,) (10000, 784) (10000,) (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "#load MINST images\n",
    "\n",
    "from mnist import MNIST\n",
    "import numpy as np\n",
    "\n",
    "mnist_loader = MNIST('/content/drive/My Drive/Colab Notebooks/datasets/MNIST')\n",
    "train_data, train_label = mnist_loader.load_training()\n",
    "test_data, test_label = mnist_loader.load_testing()\n",
    "X_train = np.array(train_data, dtype='float')/255.0 # norm to [0,1]\n",
    "y_train = np.array(train_label, dtype='short')\n",
    "X_test = np.array(test_data, dtype='float')/255.0 # norm to [0,1]\n",
    "y_test = np.array(test_label, dtype='short')\n",
    "\n",
    "# convert MNIST training labels into 10-D one-hot vectors \n",
    "Y_train = np.zeros((y_train.size, y_train.max()+1))\n",
    "Y_train[np.arange(y_train.size),y_train] = 1\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MzlXRoWcmryw",
    "outputId": "9938e2e3-9f33-4963-8828-d99545eb70e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.24301987\n",
      "Iteration 2, loss = 0.08595058\n",
      "Iteration 3, loss = 0.05706552\n",
      "Iteration 4, loss = 0.03962371\n",
      "Iteration 5, loss = 0.02908247\n",
      "Iteration 6, loss = 0.01994459\n",
      "Iteration 7, loss = 0.01544610\n",
      "Iteration 8, loss = 0.01387832\n",
      "Iteration 9, loss = 0.00784194\n",
      "Iteration 10, loss = 0.00540533\n",
      "Iteration 11, loss = 0.00432002\n",
      "Iteration 12, loss = 0.00172743\n",
      "Iteration 13, loss = 0.00105620\n",
      "Iteration 14, loss = 0.00099268\n",
      "Iteration 15, loss = 0.00071931\n",
      "Iteration 16, loss = 0.00064104\n",
      "Iteration 17, loss = 0.00060419\n",
      "Iteration 18, loss = 0.00058721\n",
      "Iteration 19, loss = 0.00057276\n",
      "Iteration 20, loss = 0.00055686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 1.000\n",
      "Test set score: 0.985\n"
     ]
    }
   ],
   "source": [
    "# build 10-digit classifer using MLPClassifier from scikit-learn\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# here are all hyper-parameters for MLPClassifier\n",
    "#\n",
    "mlp = MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "              hidden_layer_sizes=(500,250), learning_rate='constant',\n",
    "              learning_rate_init=0.1, max_fun=15000, max_iter=20, momentum=0.9,\n",
    "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
    "              random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
    "              validation_fraction=0.1, verbose=10, warm_start=False)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Training set score: {mlp.score(X_train, y_train):.3f}\")\n",
    "print(f\"Test set score: {mlp.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XI5eMDNNcSlA"
   },
   "source": [
    "## **II. Fully Connected Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV_6L86Ivt0u"
   },
   "source": [
    "### **Example 5.2:**\n",
    "\n",
    "*Implement your own fully connected neural networks from scratch. Use the stochastic gradient descent (SGD) to implement the error-backpropagation learning algorithm.  Use your implementation to build a classifier to recognize all ten digits in the MNIST data set. Compare your own implementation with that of scikit-learn in terms of accuracy and running speed.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maCaYnTzcT3M"
   },
   "source": [
    "In this implementation, we strictly follow the description and derived formula of FCNNs in [MLF]. In other words, we implement the mini-batch SGD in Algorithm 8.8 for model updating, and the forward pass on page 166 and backward pass on page 188 to compute gradients for model parameters. \n",
    "\n",
    "In our implementation, we support a flexible way that uses a python list to specify any model structure for FCNNs. For example, besides an input layer  and an output layer whose sizes are determined by input and output dimensions, if we specify *'struct = [100]'*, it means one more hidden layer of 100 nodes is added in between.  Similarly, *'struct = [500, 300, 100]'* is meant to add three more hidden layers of 500, 300, 100 nodes at each layer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsjCchgyh0x6"
   },
   "source": [
    "In the backward pass for a mini-batch, we need to compute many outer products for all vectors in each mini-batch. For example, in any hidden layer, we have error signals $\\{\\mathbf{e}_1, \\mathbf{e}_2, \\cdots,  \\mathbf{e}_{\\tiny B}\\}$ for all samples in the mini-batch, and their corresponding outputs from previous layer as  $\\{\\mathbf{z}_1, \\mathbf{z}_2, \\cdots,  \\mathbf{z}_{\\tiny B}\\}$, as shown in the backward pass on page 188, we need to compute the following outer  products :\n",
    "$$\n",
    "\\mathbf{e}_1 \\mathbf{z}_1^\\intercal, \\mathbf{e}_2 \\mathbf{z}_2^\\intercal, \\cdots, \\mathbf{e}_{\\tiny B} \\mathbf{z}_{\\tiny B}^\\intercal\n",
    "$$\n",
    "\n",
    "In order to compute these outer products via vectorization, we use a handy method in *numpy*, i.e. [the Einstein summation](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html). If we pack all error signals row by row as a $B \\times O$ matrix $\\mathbf{E}$, and outputs row by row as a $B \\times I$ matrix $\\mathbf{Z}$, the above outer products can be computed by one line of vectorized codes as follows:\n",
    "```\n",
    "numpy.einsum('bo,bi->bio', E, Z)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExKffDi12Onm"
   },
   "outputs": [],
   "source": [
    "# use numpy to implement fully-connected neural networks with mini-batch SGD \n",
    "import numpy as np\n",
    "\n",
    "# ReLU activation function\n",
    "def relu(x):\n",
    "  return np.maximum(x, 0) \n",
    "\n",
    "# column-wise softmax\n",
    "# X[N,d]: softmax over d\n",
    "def softmax(X):\n",
    "  m = X.max(axis=1)\n",
    "  return np.transpose(np.exp(X.T-m)/np.sum(np.exp(X.T-m), axis=0))\n",
    "\n",
    "class myMLP1():\n",
    "  def __init__(self, optimizer='sgd', debug=0, struct=[], activation='relu',\\\n",
    "               loss='ce', lr=1.0, max_epochs=10, batch_size=10, random_state=1,\\\n",
    "               init_range=1.0, annealing=1.0):\n",
    "    self.optimizer = optimizer     # which optimizer is used to learn\n",
    "    self.lr = lr                   # initial learning rate in SGD\n",
    "    self.annealing = annealing     # annealing rate in SGD\n",
    "    self.max_epochs = max_epochs   # max epochs in optimization \n",
    "    self.batch_size = batch_size   # mini-batch size in SGD\n",
    "    self.debug = debug             # whether print debugging info\n",
    "    self.activation=activation     # activation function \n",
    "    self.loss = loss               # the loss used for training objective \n",
    "    self.random_state=random_state # random state\n",
    "    self.init_range=init_range     # range for initializing weights \n",
    "\n",
    "    self.struct = struct           # network structure: e.g. [100], [500, 200], \n",
    "                                   #                         [100,100,100]\n",
    " \n",
    "  # initialize internal struct/variables for input/output \n",
    "  # X[N,d]: input features; Y[N,K]: 1-of-K one-hot vectors for output targets \n",
    "  def initialization(self, X, Y):\n",
    "    np.random.seed(self.random_state)\n",
    "\n",
    "    input = X.shape[1]                # input dimension \n",
    "    self.layers = len(self.struct)    # number of hidden layers \n",
    "    self.W = [0]*(self.layers+1)      # list for all weight matrices\n",
    "    self.b = [0]*(self.layers+1)      # list for all bias vectors \n",
    "    self.W_grad = [0]*(self.layers+1) # list for weight gradients\n",
    "    self.b_grad = [0]*(self.layers+1) # list for bias gradients \n",
    "\n",
    "    # create weight matrices for all hidden layers \n",
    "    for l in range(self.layers):  \n",
    "      output = self.struct[l]\n",
    "      self.W[l] = 4.90*(np.random.rand(input, output)-0.5)*self.init_range/np.sqrt(output+input)\n",
    "      self.b[l] = np.zeros(output)\n",
    "      self.W_grad[l] = np.random.rand(input, output)\n",
    "      self.b_grad[l] = np.zeros(output)\n",
    "      input = output \n",
    "\n",
    "    # create weight matrix for output layer\n",
    "    output = Y.shape[1]\n",
    "    self.W[self.layers] = 4.90*(np.random.rand(input, output)-0.5)*self.init_range/np.sqrt(output+input)\n",
    "    self.b[self.layers] = np.zeros(output)\n",
    "    self.W_grad[self.layers] = np.random.rand(input, output)\n",
    "    self.b_grad[self.layers] = np.zeros(output)\n",
    "\n",
    "\n",
    "  # forward pass to compute outputs for a mini-batch X\n",
    "  # if return_Z=True, also save all hidden activation \n",
    "  # (refer to the box on page 166)\n",
    "  # input =>  X[B,d]: a batch of input vectors\n",
    "  # if return_Z=False, return only y[B,K]\n",
    "  # otherwise, return activations for all layers (including hidden layers) \n",
    "  def forward(self, X, return_Z=False):\n",
    "    # list to save all hidden nodes' activation values \n",
    "    if (return_Z):\n",
    "      Zs = [0] * (self.layers+2)\n",
    "    else:\n",
    "      Zs = [0]\n",
    "\n",
    "    Z = X\n",
    "    if(return_Z):\n",
    "        Zs[0] = Z\n",
    "    # forward pass from all hidden layers\n",
    "    for l in range(self.layers): \n",
    "      Z = relu(Z @ self.W[l]  + self.b[l])\n",
    "      if(return_Z):\n",
    "        Zs[l+1] = Z\n",
    "\n",
    "    #forward pass for output layer\n",
    "    l = self.layers\n",
    "    y = softmax(Z @ self.W[l] + self.b[l])\n",
    "    if (return_Z):\n",
    "      Zs[l+1] = y\n",
    "    else:\n",
    "      Zs[0] = y\n",
    "    \n",
    "    return Zs\n",
    "\n",
    "  # backward pass to compute gradients for a mini-batch of inputs X and targets Y\n",
    "  # Zs: list of all hidden activation values (pre-computed by a forward pass)\n",
    "  # return gradients of all weight matrices and bias vectors \n",
    "  # (refer to the box on page 188)\n",
    "  def backward(self, X, Y, Zs):\n",
    " \n",
    "    # output layer\n",
    "    l = len(Zs)-1\n",
    "    e = Zs[l] - Y  # error signals for output layer \n",
    "    WG = np.einsum('bo,bi->bio', e, Zs[l-1])\n",
    "    self.W_grad[l-1] = np.mean(WG,axis=0) \n",
    "    self.b_grad[l-1] = np.mean(e,axis=0)\n",
    "\n",
    "    # backward for all hidden layers\n",
    "    for l in range(self.layers,0,-1):\n",
    "      e = ( e @ self.W[l].T ) * np.heaviside(Zs[l],0) \n",
    "      WG = np.einsum('bo,bi->bio', e, Zs[l-1])\n",
    "      self.W_grad[l-1] = np.mean(WG,axis=0)\n",
    "      self.b_grad[l-1] = np.mean(e,axis=0)\n",
    "\n",
    "    return self.W_grad, self.b_grad\n",
    "\n",
    "  # mini-batch SGD to update model parameters (Algorith 8.8 on page 189) \n",
    "  # X[N,d]: input feature vectors; Y[N,K]: one-hot output targets\n",
    "  def sgd(self, X, Y):\n",
    "    n = X.shape[0]            # number of samples\n",
    "\n",
    "    lr = self.lr\n",
    "    errorsA = np.zeros(self.max_epochs)\n",
    "    errorsC = np.zeros(self.max_epochs)\n",
    "\n",
    "    for epoch in range(self.max_epochs):\n",
    "      indices = np.random.permutation(n)  #randomly shuffle data indices\n",
    "      for batch_start in range(0, n, self.batch_size):\n",
    "        X_batch = X[indices[batch_start:batch_start + self.batch_size]]\n",
    "        Y_batch = Y[indices[batch_start:batch_start + self.batch_size]]\n",
    "\n",
    "        Zs = self.forward(X_batch, return_Z=True)\n",
    "\n",
    "        W_grad, b_grad = self.backward(X_batch, Y_batch, Zs)\n",
    "\n",
    "        for l in range(self.layers+1):\n",
    "          self.W[l] -= lr * W_grad[l]\n",
    "          self.b[l] -= lr * b_grad[l]\n",
    "\n",
    "      # plot all learning curves (A, B, C)\n",
    "      Z = self.forward(X, return_Z=False)\n",
    "      errorsC[epoch] = -np.mean (np.log(Z[0][Y==1]))\n",
    "\n",
    "      train_label = np.argmax(Y, axis=1)\n",
    "      train_res = np.argmax(Z[0], axis=1)\n",
    "      errorsA[epoch] = np.count_nonzero(np.equal(train_res,train_label))/train_label.size\n",
    "\n",
    "      if(self.debug):\n",
    "        print(f'epoch = {epoch} (lr={lr:.2}): C = {errorsC[epoch]:.5f}  A = {100*errorsA[epoch]:.2f}%')\n",
    "\n",
    "      lr *= self.annealing\n",
    "\n",
    "    return errorsA, errorsC\n",
    "\n",
    "  # X[N,d]: input feature vectors; Y[N,K]: one-hot output targets          \n",
    "  def fit(self, X, Y):\n",
    "\n",
    "    self.initialization(X, Y)\n",
    "\n",
    "    errorsA, errorsC = self.sgd(X, Y)\n",
    "\n",
    "    return errorsA, errorsC\n",
    "\n",
    "  # X[N,d]: input features;\n",
    "  # return: labels (NOT one-hot)\n",
    "  def predict(self, X):\n",
    "    Y = self.forward(X,return_Z=False)\n",
    "    return np.argmax(Y[0], axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "ZDCWf_t_JCrV",
    "outputId": "07ce7056-9f87-4134-c01e-89115759342e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0 (lr=0.1): C = 0.10536  A = 96.55%\n",
      "epoch = 1 (lr=0.09): C = 0.04953  A = 98.42%\n",
      "epoch = 2 (lr=0.081): C = 0.04733  A = 98.45%\n",
      "epoch = 3 (lr=0.073): C = 0.01478  A = 99.59%\n",
      "epoch = 4 (lr=0.066): C = 0.01039  A = 99.71%\n",
      "epoch = 5 (lr=0.059): C = 0.00591  A = 99.86%\n",
      "epoch = 6 (lr=0.053): C = 0.00259  A = 99.97%\n",
      "epoch = 7 (lr=0.048): C = 0.00130  A = 99.99%\n",
      "epoch = 8 (lr=0.043): C = 0.00081  A = 100.00%\n",
      "epoch = 9 (lr=0.039): C = 0.00059  A = 100.00%\n",
      "epoch = 10 (lr=0.035): C = 0.00052  A = 100.00%\n",
      "epoch = 11 (lr=0.031): C = 0.00045  A = 100.00%\n",
      "test accuracy = 98.50%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mlp = myMLP1(struct=[500,250], debug=1, max_epochs=12, annealing=0.9, batch_size=10,lr=0.1)\n",
    "\n",
    "A, C = mlp.fit(X_train, Y_train)\n",
    "\n",
    "result = mlp.predict(X_test)\n",
    "test_acc =  100.0*jnp.count_nonzero(jnp.equal(result, y_test))/y_test.size\n",
    "print(f'test accuracy = {test_acc:.2f}%')\n",
    "\n",
    "fig, ax = plt.subplots(2)\n",
    "fig.suptitle('monitoring three learning curves (A, C)')\n",
    "ax[0].plot(C, 'b')\n",
    "_=ax[0].legend(['curve C'])\n",
    "\n",
    "ax[1].plot(A, 'r')\n",
    "_=ax[1].legend(['curve A'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MWVP_cpcZAs"
   },
   "source": [
    "## **III. Running Neural Networks on GPUs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COqZFy-tWKye"
   },
   "source": [
    "### **Example 5.3:**\n",
    "\n",
    "*Re-implement the above fully connected neural networks using JAX so that we can run on GPUs for much faster training and testing speeds. Compare your JAX implementation with the above numpy codes in terms of classification accuracy and running speed.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isxyvjyGnAkR"
   },
   "source": [
    "*JAX* re-implements pretty much all *numpy* functions in its module *jax.numpy* and almost all *jax.numpy* functions adopt the same names and syntax as their *numpy* counterparts (with only a small number of exceptions).  Thus, it is straightforward to modify the above *numpy* codes into a JAX version by replacing all *numpy* functions with their *jax.numpy* counterparts. \n",
    "\n",
    "In this implementation, we have merged each weight matrix and its bias vector into a single matrix by expanding one more dimension of a constant '1' in its input vectors (as explained in the margin note on page 107). By doing so, we may be able to explore multiple GPU cores in a better way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdHBIomvbxpT"
   },
   "outputs": [],
   "source": [
    "# implement fully-connected neural networks using JAX \n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from jax import random, device_put\n",
    "\n",
    "class myMLP2():\n",
    "  def __init__(self, optimizer='sgd', debug=0, struct=[], activation='relu',\\\n",
    "               loss='ce', lr=1.0, max_epochs=10, batch_size=10, random_state=1,\\\n",
    "               init_range=1.0, annealing=1.0):\n",
    "    self.optimizer = optimizer     # which optimizer is used to learn\n",
    "    self.lr = lr                   # initial learning rate in SGD\n",
    "    self.annealing = annealing     # annealing rate in SGD\n",
    "    self.max_epochs = max_epochs   # max epochs in optimization \n",
    "    self.batch_size = batch_size   # mini-batch size in SGD\n",
    "    self.debug = debug             # whether print debugging info\n",
    "    self.activation=activation     # activation function \n",
    "    self.loss = loss               # the loss used for training objective \n",
    "    self.random_state=random_state # random state\n",
    "    self.init_range=init_range     # range for initializing weights \n",
    "\n",
    "    self.struct = struct           # network structure: e.g. [100], [500, 200], \n",
    "                                   #                         [100,100,100]\n",
    " \n",
    "  # initialize internal struct/variables for input/output \n",
    "  # X[N,d]: input features; Y[N,K]: 1-of-K one-hot vectors for output targets \n",
    "  def initialization(self, X, Y):\n",
    "    key = random.PRNGKey(self.random_state)\n",
    "\n",
    "    input = X.shape[1]                # input dimension \n",
    "    self.layers = len(self.struct)    # number of hidden layers \n",
    "    self.W_b = [0]*(self.layers+1)      # list for all weight matrices\n",
    "    self.W_b_grad = [0]*(self.layers+1) # list for weight gradients\n",
    "\n",
    "    # create weight matrices for all hidden layers \n",
    "    for l in range(self.layers):  \n",
    "      output = self.struct[l]\n",
    "      self.W_b[l] = device_put(4.90*(random.uniform(key,(input+1, output))-0.5)*self.init_range/jnp.sqrt(output+input))\n",
    "      self.W_b_grad[l] = device_put(jnp.zeros((input+1, output)))\n",
    "      input = output \n",
    "\n",
    "    # create weight matrix for output layer\n",
    "    output = Y.shape[1]\n",
    "    self.W_b[self.layers] = device_put(4.90*(random.uniform(key,(input+1, output))-0.5)*self.init_range/jnp.sqrt(output+input))\n",
    "    self.W_b_grad[self.layers] = device_put(jnp.zeros((input+1, output)))\n",
    "\n",
    "    return\n",
    "\n",
    "  # forward pass to compute outputs for a mini-batch X\n",
    "  # if return_Z=True, also save all hidden activation \n",
    "  # (refer to the box on page 166)\n",
    "  # input =>  X[B,d]: a batch of input vectors\n",
    "  # if return_Z=False, return only y[B,K]\n",
    "  # otherwise, return activations for all layers (including hidden layers) \n",
    "  def forward(self, W_b, X, return_Z=False):\n",
    "    # list to save all hidden nodes' activation values \n",
    "    if (return_Z):\n",
    "      Zs = [0] * (self.layers+2)\n",
    "    else:\n",
    "      Zs = [0]\n",
    "\n",
    "    # appending 1's to accomodate bias (see page 107)\n",
    "    Z = jnp.hstack((X,jnp.ones((X.shape[0],1),dtype=X.dtype)))\n",
    "    if(return_Z):\n",
    "        Zs[0] = Z\n",
    "    # forward pass from all hidden layers\n",
    "    for l in range(self.layers): \n",
    "      Z = jnn.relu(Z @ W_b[l])\n",
    "      Z = jnp.hstack((Z,jnp.ones((Z.shape[0],1),dtype=Z.dtype)))\n",
    "      if(return_Z):\n",
    "        Zs[l+1] = Z\n",
    "\n",
    "    #forward pass for output layer\n",
    "    l = self.layers\n",
    "    y = jnn.softmax(Z @ W_b[l], axis=1)\n",
    "    if (return_Z):\n",
    "      Zs[l+1] = y\n",
    "    else:\n",
    "      Zs[0] = y\n",
    "    \n",
    "    return Zs\n",
    "\n",
    "  # backward pass to compute gradients for a mini-batch of inputs X and targets Y\n",
    "  # Zs: list of all hidden activation values (pre-computed in a forward pass)\n",
    "  # return gradients of all weight matrices and bias vectors \n",
    "  # (refer to the box on page 188)\n",
    "  def backward(self, X, Y, Zs):\n",
    " \n",
    "    # output layer\n",
    "    l = len(Zs)-1\n",
    "    e = Zs[l] - Y  # error signals for output layer \n",
    "    WG = jnp.einsum('bo,bi->bio', e, Zs[l-1])\n",
    "    self.W_b_grad[l-1] = jnp.mean(WG,axis=0) \n",
    "\n",
    "    # backward for all hidden layers\n",
    "    for l in range(self.layers,0,-1):\n",
    "      e = ( e @ self.W_b[l].T ) * jnp.heaviside(Zs[l],0)\n",
    "      e = jnp.delete(e, -1, axis=1) # remove the column related to contant '1'\n",
    "\n",
    "      WG = jnp.einsum('bo,bi->bio', e, Zs[l-1])\n",
    "      self.W_b_grad[l-1] = jnp.mean(WG,axis=0)\n",
    " \n",
    "    return \n",
    "\n",
    "  # compute the CE loss for a mini-batch\n",
    "  # W_b[ ]: list for all weight matrices\n",
    "  # X[B,d]: input features; \n",
    "  # Y[B,K]: 1-of-K one-hot vectors for output targets\n",
    "  def loss_ce_batch(self, W_b, X, Y):\n",
    "    R = self.forward(W_b, X, return_Z=False)\n",
    "\n",
    "    return -jnp.mean(jnp.log(R[0][Y==1]))\n",
    "\n",
    "  # mini-batch SGD to update model parameters (Algorith 8.8 on page 189) \n",
    "  # X[N,d]: input feature vectors; Y[N,K]: one-hot output targets\n",
    "  def sgd(self, X, Y):\n",
    "    n = X.shape[0]            # number of samples\n",
    "\n",
    "    lr = self.lr\n",
    "    errorsA = np.zeros(self.max_epochs)\n",
    "    errorsC = np.zeros(self.max_epochs)\n",
    "\n",
    "    for epoch in range(self.max_epochs):\n",
    "      indices = np.random.permutation(n)  #randomly shuffle data indices\n",
    "      for batch_start in range(0, n, self.batch_size):\n",
    "        X_batch = X[indices[batch_start:batch_start + self.batch_size]]\n",
    "        Y_batch = Y[indices[batch_start:batch_start + self.batch_size]]\n",
    "\n",
    "        Zs = self.forward(self.W_b, X_batch, return_Z=True)\n",
    "\n",
    "        self.backward(X_batch, Y_batch, Zs)\n",
    "\n",
    "        for l in range(self.layers+1):\n",
    "          self.W_b[l] -= lr * self.W_b_grad[l]\n",
    "          \n",
    "      # plot all learning curves (A and C)\n",
    "      errorsC[epoch] = self.loss_ce_batch(self.W_b, X, Y)\n",
    "\n",
    "      result = self.predict(X)\n",
    "      errorsA[epoch] =  jnp.count_nonzero(jnp.equal(result, y_train))/y_train.size\n",
    " \n",
    "      if(self.debug):\n",
    "        print(f'epoch = {epoch} (lr={lr:.2}): C = {errorsC[epoch]:.5f}  A = {100*errorsA[epoch]:.2f}%')\n",
    "\n",
    "      lr *= self.annealing\n",
    "\n",
    "    return errorsA, errorsC\n",
    "\n",
    " # X[N,d]: input feature vectors; Y[N,K]: one-hot output targets         \n",
    "  def fit(self, X, Y):\n",
    "\n",
    "    self.initialization(X, Y)\n",
    "\n",
    "    X2 = device_put(X)\n",
    "    Y2 = device_put(Y)\n",
    "\n",
    "    errorsA, errorsC = self.sgd(X2, Y2)\n",
    "\n",
    "    return errorsA, errorsC\n",
    "\n",
    "  # X[N,d]: input features;\n",
    "  # return: labels (NOT one-hot)\n",
    "  def predict(self, X):\n",
    "    X2 = device_put(X)\n",
    "    Y = self.forward(self.W_b, X2)\n",
    "    return jnp.argmax(Y[0], axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "2ONGCB12arnD",
    "outputId": "f577d6af-04be-40af-91c7-8bc719ccf81a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0 (lr=0.1): C = 0.09220  A = 97.22%\n",
      "epoch = 1 (lr=0.09): C = 0.04670  A = 98.52%\n",
      "epoch = 2 (lr=0.081): C = 0.03381  A = 98.94%\n",
      "epoch = 3 (lr=0.073): C = 0.01755  A = 99.48%\n",
      "epoch = 4 (lr=0.066): C = 0.01634  A = 99.48%\n",
      "epoch = 5 (lr=0.059): C = 0.00456  A = 99.90%\n",
      "epoch = 6 (lr=0.053): C = 0.00245  A = 99.95%\n",
      "epoch = 7 (lr=0.048): C = 0.00167  A = 99.98%\n",
      "epoch = 8 (lr=0.043): C = 0.00086  A = 99.99%\n",
      "epoch = 9 (lr=0.039): C = 0.00063  A = 100.00%\n",
      "epoch = 10 (lr=0.035): C = 0.00054  A = 100.00%\n",
      "epoch = 11 (lr=0.031): C = 0.00048  A = 100.00%\n",
      "test accuracy = 98.63%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mlp = myMLP2(struct=[500,250], debug=1, max_epochs=12, annealing=0.9, batch_size=10,lr=0.1)\n",
    "\n",
    "A, C = mlp.fit(X_train, Y_train)\n",
    "\n",
    "result = mlp.predict(X_test)\n",
    "\n",
    "test_acc =  100.0*jnp.count_nonzero(jnp.equal(result, y_test))/y_test.size\n",
    "print(f'test accuracy = {test_acc:.2f}%')\n",
    "\n",
    "fig, ax = plt.subplots(2)\n",
    "fig.suptitle('monitoring three learning curves (A, C)')\n",
    "ax[0].plot(C, 'b')\n",
    "_=ax[0].legend(['curve C'])\n",
    "\n",
    "ax[1].plot(A, 'r')\n",
    "_=ax[1].legend(['curve A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs8C2Ck4QZkH",
    "outputId": "b607a053-ed39-48a7-fcba-b08474a5c6da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan 28 16:46:37 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# show the GPU type used in the above computation\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmDazwabZlTb"
   },
   "source": [
    "### **Example 5.4:**\n",
    "\n",
    "*Re-implement the above fully connected neural networks using JAX and its automatic differenttiation function jax.grad() so that you do not need to explicitly implement error back-propagation on your own. Compare this implementation with those in the previous examples and discuss the advantages to use automatic differentiation in implementing machine learning models.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEdsj_Hw6DO-"
   },
   "source": [
    "Automatic differentiation is a convenient approach to implement many machine learning methods because it can ease us from lots of tedious derivations and implementations related to how to compute gradients for the models. In this case, we only need to specify the forward pass based on the model structure and then define an objective function according to the forward pass and a loss function. After that, we can use automatic differentiation, e.g. [*jax.grad()*](https://jax.readthedocs.io/en/latest/jax.html#jax.grad), to automatically compute the gradients w.r.t. model parameters, and directly run any gradient descent optimization method to update the model with the automatically derived gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08PQI-MfaoT1"
   },
   "outputs": [],
   "source": [
    "# implement fully-connected neural networks using JAX and jax.grad()\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from jax import grad, random, device_put\n",
    "\n",
    "class myMLP3():\n",
    "  def __init__(self, optimizer='sgd', debug=0, struct=[], activation='relu',\\\n",
    "               loss='ce', lr=1.0, max_epochs=10, batch_size=10, random_state=1,\\\n",
    "               init_range=1.0, annealing=1.0):\n",
    "    self.optimizer = optimizer     # which optimizer is used to learn\n",
    "    self.lr = lr                   # initial learning rate in SGD\n",
    "    self.annealing = annealing     # annealing rate in SGD\n",
    "    self.max_epochs = max_epochs   # max epochs in optimization \n",
    "    self.batch_size = batch_size   # mini-batch size in SGD\n",
    "    self.debug = debug             # whether print debugging info\n",
    "    self.activation=activation     # activation function \n",
    "    self.loss = loss               # the loss used for training objective \n",
    "    self.random_state=random_state # random state\n",
    "    self.init_range=init_range     # range for initializing weights \n",
    "\n",
    "    self.struct = struct           # network structure: e.g. [100], [500, 200], \n",
    "                                   #                         [100,100,100]\n",
    " \n",
    "  # initialize internal struct/variables for input/output \n",
    "  # X[N,d]: input features; Y[N,K]: 1-of-K one-hot vectors for output targets \n",
    "  def initialization(self, X, Y):\n",
    "    key = random.PRNGKey(self.random_state)\n",
    "\n",
    "    input = X.shape[1]                # input dimension \n",
    "    self.layers = len(self.struct)    # number of hidden layers \n",
    "    self.W_b = [0]*(self.layers+1)    # list for all weight matrices\n",
    "\n",
    "    # create weight matrices for all hidden layers \n",
    "    for l in range(self.layers):  \n",
    "      output = self.struct[l]\n",
    "      self.W_b[l] = device_put(4.90*(random.uniform(key,(input+1, output))-0.5)*self.init_range/jnp.sqrt(output+input))\n",
    "      input = output \n",
    "\n",
    "    # create weight matrix for output layer\n",
    "    output = Y.shape[1]\n",
    "    self.W_b[self.layers] = device_put(4.90*(random.uniform(key,(input+1, output))-0.5)*self.init_range/jnp.sqrt(output+input))\n",
    "\n",
    "    return\n",
    "\n",
    "  # forward pass to compute outputs for a mini-batch X\n",
    "  # (refer to the box on page 166)\n",
    "  # input  =>  X[B,d]: a batch of input vectors\n",
    "  # return =>  y[B,K]\n",
    "  def forward(self, W_b, X):\n",
    "\n",
    "    # appending 1's to accomodate bias (see page 107)\n",
    "    Z = jnp.hstack((X,jnp.ones((X.shape[0],1),dtype=X.dtype)))\n",
    "\n",
    "    # forward pass from all hidden layers\n",
    "    for l in range(self.layers): \n",
    "      Z = jnn.relu(Z @ W_b[l])\n",
    "      Z = jnp.hstack((Z,jnp.ones((Z.shape[0],1),dtype=Z.dtype)))\n",
    "\n",
    "    # forward pass for output layer\n",
    "    l = self.layers\n",
    "    y = jnn.softmax(Z @ W_b[l], axis=1)\n",
    "    \n",
    "    return y\n",
    "\n",
    "  # compute the CE loss for a mini-batch\n",
    "  # W_b[ ]: list for all weight matrices\n",
    "  # X[B,d]: input features; \n",
    "  # Y[B,K]: 1-of-K one-hot vectors for output targets\n",
    "  def loss_ce_batch(self, W_b, X, Y):\n",
    "    R = self.forward(W_b, X)\n",
    "\n",
    "    return -jnp.mean(jnp.log(R[Y==1]))\n",
    "\n",
    "  # use minibatch SGD to optimize (refer to Algorithm 8.8 on page 189)\n",
    "  # X[N,d]: input features; Y[N,K]: 1-of-K one-hot vectors for output targets\n",
    "  def sgd(self, X, Y):\n",
    "    n = X.shape[0]      # number of samples\n",
    "\n",
    "    lr = self.lr\n",
    "    errorsA = np.zeros(self.max_epochs)\n",
    "    #errorsB = np.zeros(self.max_epochs)\n",
    "    errorsC = np.zeros(self.max_epochs)\n",
    "\n",
    "    for epoch in range(self.max_epochs):\n",
    "      indices = np.random.permutation(n)  #randomly shuffle data indices\n",
    "      for batch_start in range(0, n, self.batch_size):\n",
    "        X_batch = X[indices[batch_start:batch_start + self.batch_size]]\n",
    "        Y_batch = Y[indices[batch_start:batch_start + self.batch_size]]\n",
    "\n",
    "        W_b_grad = grad(self.loss_ce_batch)(self.W_b, X_batch, Y_batch)\n",
    "\n",
    "        for l in range(self.layers+1):\n",
    "          self.W_b[l] -= lr * W_b_grad[l]\n",
    "\n",
    "      # plot all learning curves (A, B, C)\n",
    "      errorsC[epoch] = self.loss_ce_batch(self.W_b, X, Y)\n",
    "\n",
    "      Z = self.forward(self.W_b, X)\n",
    "      train_label = np.argmax(Y, axis=1)\n",
    "      train_res = np.argmax(Z, axis=1)\n",
    "      errorsA[epoch] = np.count_nonzero(np.equal(train_res,train_label))/train_label.size\n",
    "\n",
    "      if(self.debug):\n",
    "        print(f'epoch = {epoch} (lr={lr:.2}): C = {errorsC[epoch]:.5f}  A = {100*errorsA[epoch]:.2f}%')\n",
    "\n",
    "      lr *= self.annealing\n",
    "\n",
    "    return errorsA, errorsC\n",
    "\n",
    "   # X[N,d]: input features; Y[N,K]: 1-of-K one-hot vectors for output targets         \n",
    "  def fit(self, X, Y):\n",
    "    # initialize all weight matrices \n",
    "    self.initialization(X, Y)\n",
    "\n",
    "    X2 = device_put(X)\n",
    "    Y2 = device_put(Y)\n",
    "\n",
    "    errorsA, errorsC = self.sgd(X2, Y2)\n",
    "\n",
    "    return errorsA, errorsC\n",
    "\n",
    "  # X[N,d]: input features;\n",
    "  # return: labels \n",
    "  def predict(self, X):\n",
    "    X2 = device_put(X)\n",
    "    Y = self.forward(self.W_b, X2)\n",
    "    return jnp.argmax(Y, axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 658
    },
    "id": "-Wdtuo-QpkxV",
    "outputId": "d917e4d4-3c00-4a8e-e895-5a8bf33b8bbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0 (lr=0.1): C = 0.13020  A = 96.16%\n",
      "epoch = 1 (lr=0.098): C = 0.07862  A = 97.77%\n",
      "epoch = 2 (lr=0.096): C = 0.06679  A = 98.03%\n",
      "epoch = 3 (lr=0.094): C = 0.04262  A = 98.81%\n",
      "epoch = 4 (lr=0.092): C = 0.02852  A = 99.28%\n",
      "epoch = 5 (lr=0.09): C = 0.02375  A = 99.39%\n",
      "epoch = 6 (lr=0.089): C = 0.02096  A = 99.48%\n",
      "epoch = 7 (lr=0.087): C = 0.01511  A = 99.72%\n",
      "epoch = 8 (lr=0.085): C = 0.01241  A = 99.73%\n",
      "epoch = 9 (lr=0.083): C = 0.00811  A = 99.89%\n",
      "epoch = 10 (lr=0.082): C = 0.00668  A = 99.92%\n",
      "epoch = 11 (lr=0.08): C = 0.00487  A = 99.96%\n",
      "epoch = 12 (lr=0.078): C = 0.00394  A = 99.98%\n",
      "epoch = 13 (lr=0.077): C = 0.00305  A = 99.99%\n",
      "epoch = 14 (lr=0.075): C = 0.00279  A = 99.99%\n",
      "epoch = 15 (lr=0.074): C = 0.00234  A = 100.00%\n",
      "epoch = 16 (lr=0.072): C = 0.00195  A = 100.00%\n",
      "epoch = 17 (lr=0.071): C = 0.00177  A = 100.00%\n",
      "epoch = 18 (lr=0.07): C = 0.00169  A = 100.00%\n",
      "epoch = 19 (lr=0.068): C = 0.00148  A = 100.00%\n",
      "test accuracy = 98.27%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mlp = myMLP3(struct=[500,250], debug=1, max_epochs=20, annealing=0.98, batch_size=50,lr=0.1)\n",
    "\n",
    "A, C = mlp.fit(X_train, Y_train)\n",
    "\n",
    "result = mlp.predict(X_test)\n",
    "\n",
    "test_acc =  100.0*jnp.count_nonzero(jnp.equal(result, y_test))/y_test.size\n",
    "print(f'test accuracy = {test_acc:.2f}%')\n",
    "\n",
    "fig, ax = plt.subplots(2)\n",
    "fig.suptitle('monitoring three learning curves (A, C)')\n",
    "ax[0].plot(C, 'b')\n",
    "_=ax[0].legend(['curve C'])\n",
    "\n",
    "ax[1].plot(A, 'r')\n",
    "_=ax[1].legend(['curve A'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQjMIubFKr4a"
   },
   "source": [
    "In the following, we compare the total running times  (of 3 training epochs) among all three implementaions. As we can see, the JAX implemetation in Example 5.3 (running in GPUs) yields the fastest speed while the JAX autograd implementation in Example 5.4 (running in GPUs) is about twice slower (58.2 sec vs. 28.8 sec). On the other hand, the numpy implementation in Example 5.2 (running in CPUs) takes much longer time than those of JAX codes, about 10 times slower than the JAX implementation in Example 5.3 (running in GPUs), whose running times are measured as 293 sec vs. 28.8 sec. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ddVQpCpZAs_",
    "outputId": "49ca9487-b91a-4c0a-cde7-b8e04afae542"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy implementation running on CPUs\n",
      "1 loop, best of 5: 4min 53s per loop\n",
      "1 loop, best of 5: 333 ms per loop\n",
      "test accuracy = 96.46%\n",
      "JAX implementation running on GPUs\n",
      "1 loop, best of 5: 28.8 s per loop\n",
      "100 loops, best of 5: 17.2 ms per loop\n",
      "test accuracy = 96.82%\n",
      "JAX auto-grad implementation running on GPUs\n",
      "1 loop, best of 5: 58.2 s per loop\n",
      "100 loops, best of 5: 17.5 ms per loop\n",
      "test accuracy = 96.83%\n"
     ]
    }
   ],
   "source": [
    "# Measure and compare running times for all three different implementations \n",
    "\n",
    "print('numpy implementation running on CPUs') \n",
    "\n",
    "mlp = myMLP1(struct=[500,250], debug=0, max_epochs=3, annealing=0.99, batch_size=100,lr=0.1)\n",
    "\n",
    "%timeit errorsA, errorsC = mlp.fit(X_train, Y_train)\n",
    "\n",
    "%timeit result = mlp.predict(X_test)\n",
    "result = mlp.predict(X_test)\n",
    "test_acc =  100.0*jnp.count_nonzero(jnp.equal(result, y_test))/y_test.size\n",
    "print(f'test accuracy = {test_acc:.2f}%')\n",
    "\n",
    "print('JAX implementation running on GPUs') \n",
    "\n",
    "mlp = myMLP2(struct=[500,250], debug=0, max_epochs=3, annealing=0.99, batch_size=100,lr=0.1)\n",
    "\n",
    "%timeit errorsA, errorsC = mlp.fit(X_train, Y_train)\n",
    "\n",
    "%timeit result = mlp.predict(X_test)\n",
    "result = mlp.predict(X_test)\n",
    "test_acc =  100.0*jnp.count_nonzero(jnp.equal(result, y_test))/y_test.size\n",
    "print(f'test accuracy = {test_acc:.2f}%')\n",
    "\n",
    "print('JAX auto-grad implementation running on GPUs') \n",
    "\n",
    "mlp = myMLP3(struct=[500,250], debug=0, max_epochs=3, annealing=0.99, batch_size=100,lr=0.1)\n",
    "\n",
    "%timeit errorsA, errorsC = mlp.fit(X_train, Y_train)\n",
    "\n",
    "%timeit result = mlp.predict(X_test)\n",
    "result = mlp.predict(X_test)\n",
    "test_acc =  100.0*jnp.count_nonzero(jnp.equal(result, y_test))/y_test.size\n",
    "print(f'test accuracy = {test_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ee6_NeOtgxYc"
   },
   "source": [
    "## **Exercises**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OuW5V0iRg3ef"
   },
   "source": [
    "### **Problem 5.1:**\n",
    "\n",
    "Use automatic differentiation in *JAX*, i.e. *jax.grad()*, to re-implement logistic regression in Example 3.1, and then compare it with the *numpy* implementation in Example 3.1 in terms of classification accuracy and running speed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxWbZQ--9SWd"
   },
   "source": [
    "### **Problem 5.2:**\n",
    "\n",
    "Use automatic differentiation in *JAX* to implement FCNNs for the *autoencoder* in Figure 4.15 on page 90. Use all training images in MNIST to train the autoencoder and then use its encoder part to extract features for two digits ('3' and '8') in MNIST. Use the extracted features in the training set to train a logistic regression model as in Problem 5.1 and evaluate classification accuracy using all digits '3' and '8' in the test set. Fine-tune the autoencoder structure towards the best possible classification accuracy between '3' and '8'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItGUEqdU_C3w"
   },
   "source": [
    "### **Problem 5.3:**\n",
    "\n",
    "Use JAX automatic differentiation to implement FCNNs for the *bottleneck (BN) features* in Figure 4.16 on page 91. Use all training images in MNIST along with their labels to train the BN model and then use its encoder part to extract features for two digits ('3' and '8') in MNIST. Use the extracted features from the training set to train a logistic regression model as in Problem 5.1 and evaluate classification accuracy using digits '3' and '8' from the test set. Fine-tune the BN model structure towards the best possible classification accuracy between '3' and '8'. Compare the performance of *BN* features with that of autoencoder in Problem 5.2 and discuss the possible reason for the performance gap. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tO22jjlGD2n"
   },
   "source": [
    "### **Problem 5.4:**\n",
    "\n",
    "Expand the FCNN implementation in Example 5.3 (or Example 5.4) by adding the *ADAM* optimzer in Algorithm 8.9 (on page 192) as another optimizer option for FCNNs. Compare ADAM with SGD in terms of the convergence behavior and classification accuracy."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab5_Fully_Connected_Neural_Networks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
