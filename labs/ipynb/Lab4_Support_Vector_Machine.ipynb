{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9PdfLaJS1bL"
   },
   "source": [
    "# **Lab 4: Support Vector Machine**\n",
    "\n",
    "---\n",
    "NOTE: This is a lab project accompanying the following book [MLF] and it should be used together with the book.\n",
    "\n",
    "[MLF] *H. Jiang*, \"[Machine Learning Fundamentals: A Concise Introduction](http://wiki.eecs.yorku.ca/user/hj/research:mlfbook)\", Cambridge University Press, 2021.  ([bibtex](http://www.cse.yorku.ca/~hj/mlf-jiang.bib))\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96kRvHC9THUB"
   },
   "source": [
    "The purpose of this lab is to practise and implement an important discriminative model in machine learning, namely support vector machines (SVMs). We first introduce  the SVM implementation in *scikit-learn* and show how to use it to fine-tune hyper-parameters for several popular kernel functions in a binary classification task. Next, we will demonstrate how to use a projected gradient descent method described in [MLF] to implement SVMs from scratch, including both linear and nonlinear SVMs. We will compare our own SVM implementation with that of *scikit-learn* in terms of classification performance and running speed in a binary classification task. As a project-end exercise, we will consider how to extend binary SVMs for multiple-class classification tasks based on the *one-vs-one* strategy. \n",
    "\n",
    "Prerequisites: N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1VPiyLNdTQg5"
   },
   "source": [
    "## **I. SVMs from scikit-learn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L35HVBKDVO3l"
   },
   "source": [
    "### **Example 4.1:**\n",
    "\n",
    "*Use the SVM functions from scikit-learn to build a binary classifier to classify two digits ('3' and '8') in the MNIST data set. Fine-tune the hyper-parameters for three important kernel funcions, i.e. linear, polynomial and Gaussian RBF kernels, towards the best possible performance.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X91U2icYWSqC",
    "outputId": "f7db74d6-67ef-4c68-9e10-d844a9f4e6ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1Jf2XqGR7y1fzOZNKLJiom7GmZZUzXhfs t10k-images-idx3-ubyte\n",
      "Processing file 1qiYu9dW3ZNrlvTFO5fI4qf8Wtr8K-pCu t10k-labels-idx1-ubyte\n",
      "Processing file 1SnWvBcUETRJ53rEJozFUUo-hOQFPKxjp train-images-idx3-ubyte\n",
      "Processing file 1kKEIi_pwVHmabByAnwZQsaMgro9XiBFE train-labels-idx1-ubyte\n",
      "Building directory structure completed\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting python_mnist\n",
      "  Downloading python_mnist-0.7-py2.py3-none-any.whl (9.6 kB)\n",
      "Installing collected packages: python_mnist\n",
      "Successfully installed python_mnist-0.7\n"
     ]
    }
   ],
   "source": [
    "# download MNIST data from Google drive\n",
    "\n",
    "!gdown --folder https://drive.google.com/drive/folders/1r20aRjc2iu9O3kN3Xj9jNYY2uMgcERY1 2> /dev/null\n",
    "\n",
    "# install python_mnist\n",
    "\n",
    "!pip install python_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vjL8SfU8WW5-",
    "outputId": "6c9894e5-3eab-4333-b04e-6081f35bdc4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,) (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "#load MINST images\n",
    "\n",
    "from mnist import MNIST\n",
    "import numpy as np\n",
    "\n",
    "mnist_loader = MNIST('MNIST')\n",
    "train_data, train_label = mnist_loader.load_training()\n",
    "test_data, test_label = mnist_loader.load_testing()\n",
    "train_data = np.array(train_data, dtype='float')/255 # norm to [0,1]\n",
    "train_label = np.array(train_label, dtype='short')\n",
    "test_data = np.array(test_data, dtype='float')/255 # norm to [0,1]\n",
    "test_label = np.array(test_label, dtype='short')\n",
    "\n",
    "print(train_data.shape, train_label.shape, test_data.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77DeyvQ2Wal7"
   },
   "outputs": [],
   "source": [
    "# prepare digits '3' and '8' for binary SVMs\n",
    "\n",
    "digit_train_index = np.logical_or(train_label == 3, train_label == 8)\n",
    "X_train = train_data[digit_train_index]\n",
    "y_train = train_label[digit_train_index]\n",
    "digit_test_index = np.logical_or(test_label == 3, test_label == 8)\n",
    "X_test = test_data[digit_test_index]\n",
    "y_test = test_label[digit_test_index]\n",
    "\n",
    "# normalize all feature vectors to unit-length\n",
    "X_train = np.transpose (X_train.T / np.sqrt(np.sum(X_train*X_train, axis=1)))\n",
    "X_test =  np.transpose (X_test.T  / np.sqrt(np.sum(X_test*X_test, axis=1)))\n",
    "\n",
    "# convert labels: '3' => -1, '8' => +1\n",
    "CUTOFF = 5 # any number between '3' and '8'\n",
    "y_train = np.sign(y_train-CUTOFF)\n",
    "y_test = np.sign(y_test-CUTOFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "unUwAdOLZZ85",
    "outputId": "701fa8fc-6a46-4b36-99eb-27f8bc77235a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear SVM (C=0.01): training accuracy=94.54%  test accuracy=95.31%\n",
      "linear SVM (C=0.1): training accuracy=96.50%  test accuracy=96.98%\n",
      "linear SVM (C=1): training accuracy=97.39%  test accuracy=96.82%\n",
      "linear SVM (C=2): training accuracy=97.48%  test accuracy=96.93%\n",
      "linear SVM (C=4): training accuracy=97.60%  test accuracy=96.93%\n",
      "linear SVM (C=10): training accuracy=97.79%  test accuracy=97.08%\n"
     ]
    }
   ],
   "source": [
    "# linear SVM: use sk-learn SVC functions\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "for c in [0.01, 0.1, 1, 2, 4, 10]:\n",
    "  linearSVM = SVC(kernel='linear', C=c)\n",
    "  linearSVM.fit(X_train,y_train)\n",
    "  predict = linearSVM.predict(X_train)\n",
    "  train_acc = np.count_nonzero(np.equal(predict,y_train))/y_train.size\n",
    "  predict = linearSVM.predict(X_test)\n",
    "  test_acc = np.count_nonzero(np.equal(predict,y_test))/y_test.size\n",
    "  print(f'linear SVM (C={c}): training accuracy={100*train_acc:.2f}%  test accuracy={100*test_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gNttLc_WfkPM",
    "outputId": "531f3767-9b08-477a-efc9-5c5441fba6df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nonlinear polynomial SVM (C=1,d=2): training accuracy=99.57%  test accuracy=99.45%\n",
      "nonlinear polynomial SVM (C=1,d=3): training accuracy=99.84%  test accuracy=99.55%\n",
      "nonlinear polynomial SVM (C=2,d=2): training accuracy=99.73%  test accuracy=99.50%\n",
      "nonlinear polynomial SVM (C=2,d=3): training accuracy=99.96%  test accuracy=99.50%\n",
      "nonlinear polynomial SVM (C=4,d=2): training accuracy=99.92%  test accuracy=99.45%\n",
      "nonlinear polynomial SVM (C=4,d=3): training accuracy=99.99%  test accuracy=99.50%\n"
     ]
    }
   ],
   "source": [
    "# nonlinear SVM (polynomial kernel): use sk-learn SVC functions\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "for c in [1, 2, 4]:\n",
    "  for d in [2, 3]:\n",
    "    polySVM = SVC(kernel='poly', C=c, degree=d)\n",
    "    polySVM.fit(X_train,y_train)\n",
    "    predict = polySVM.predict(X_train)\n",
    "    train_acc = np.count_nonzero(np.equal(predict,y_train))/y_train.size\n",
    "    predict = polySVM.predict(X_test)\n",
    "    test_acc = np.count_nonzero(np.equal(predict,y_test))/y_test.size\n",
    "    print(f'nonlinear polynomial SVM (C={c},d={d}): training accuracy={100*train_acc:.2f}%  test accuracy={100*test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "et_A6puwkaza",
    "outputId": "f57a1009-10b1-45c0-d8c6-9e8b62bb28b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nonlinear RBF SVM (C=1,gamma=scale): training accuracy=99.56%  test accuracy=99.40%\n",
      "nonlinear RBF SVM (C=1,gamma=1): training accuracy=99.41%  test accuracy=99.24%\n",
      "nonlinear RBF SVM (C=1,gamma=2): training accuracy=99.79%  test accuracy=99.55%\n",
      "nonlinear RBF SVM (C=2,gamma=scale): training accuracy=99.78%  test accuracy=99.55%\n",
      "nonlinear RBF SVM (C=2,gamma=1): training accuracy=99.70%  test accuracy=99.55%\n",
      "nonlinear RBF SVM (C=2,gamma=2): training accuracy=99.96%  test accuracy=99.60%\n",
      "nonlinear RBF SVM (C=10,gamma=scale): training accuracy=100.00%  test accuracy=99.50%\n",
      "nonlinear RBF SVM (C=10,gamma=1): training accuracy=99.98%  test accuracy=99.50%\n",
      "nonlinear RBF SVM (C=10,gamma=2): training accuracy=100.00%  test accuracy=99.60%\n"
     ]
    }
   ],
   "source": [
    "# nonlinear SVM (Gaussian RBF kernel): use sk-learn SVC functions\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "for c in [1, 2, 10]:\n",
    "  for g in ['scale', 1, 2]:\n",
    "    rbfSVM = SVC(kernel='rbf', C=c, gamma=g)\n",
    "    rbfSVM.fit(X_train,y_train)\n",
    "    predict = rbfSVM.predict(X_train)\n",
    "    train_acc = np.count_nonzero(np.equal(predict,y_train))/y_train.size\n",
    "    predict = rbfSVM.predict(X_test)\n",
    "    test_acc = np.count_nonzero(np.equal(predict,y_test))/y_test.size\n",
    "    print(f'nonlinear RBF SVM (C={c},gamma={g}): training accuracy={100*train_acc:.2f}%  test accuracy={100*test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xioz_RF0Tb40"
   },
   "source": [
    "## **II. Linear SVMs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGzNfuPohM5e"
   },
   "source": [
    "### **Example 4.2:**\n",
    "\n",
    "*Implement your own linear SVM function from scratch. Use the projected gradient descent (PGD), i.e. Algorithm 6.5 on page 127, to solve quadratic programming arising from the SVM dual problem. Use your implementation to build a binary classifier to classify two digits ('3' and '8') in the MNIST data set. Compare your own implementation with that of scikit-learn in terms of accuracy and running speed.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1AwSCDMMK0S"
   },
   "source": [
    "The challenge to implement SVMs from scratch is how to implement an efficient optimization method to solve the quadratic programming in the SVM dual problem. The strict projected gradient decent (PGD) algorithm  typically converges very slowly in practice. Here we will implement a mini-batch version of PGD. At each step, instead of updating all variables in ${\\boldsymbol \\alpha}$ as in Algorithm 6.5, we first randomly select a subset of varaibles in ${\\boldsymbol \\alpha}$ (like a mini-batch in SGD), denoted as ${\\boldsymbol \\alpha}_s$, and only update all variables in ${\\boldsymbol \\alpha}_s$ using the same idea of PGD while keeping the other variables in ${\\boldsymbol \\alpha}$ unchanged. We first compute the gradient w.r.t. ${\\boldsymbol \\alpha}_s$ as follows:\n",
    "$$\n",
    "\\nabla  L({\\boldsymbol \\alpha}_s^{(n)}) = \\mathbf{Q}_s \\, {\\boldsymbol \\alpha}^{(n)} - \\mathbf{1}\n",
    "$$\n",
    "where $\\mathbf{Q}_s$ denotes a smaller matrix where we only keep the rows in $\\mathbf{Q}$ corresponding to the selected variables in ${\\boldsymbol \\alpha}_s$.\n",
    "\n",
    "Then, we project the above gradient to the subspace $ \\mathbf{y}_s^\\intercal {\\boldsymbol \\alpha}_s = 0$, where $\\mathbf{y}_s$ denotes the targets corresponding to the selected subset ${\\boldsymbol \\alpha}_s$, in the same way as in Algorithm 6.5:\n",
    "\n",
    "$$\n",
    "\\tilde{\\nabla} L({\\boldsymbol \\alpha}_s^{(n)})\n",
    "= \\nabla L({\\boldsymbol \\alpha}_s^{(n)})\n",
    "- \\frac{ \\mathbf{ y}_s^\\intercal \\nabla L({\\boldsymbol \\alpha}_s^{(n)}) }{||\\mathbf{ y}_s||^2} \\mathbf{ y}_s\n",
    "$$\n",
    "\n",
    "If we update ${\\boldsymbol \\alpha}_s$ using the projected gradient $\\tilde{\\nabla} L({\\boldsymbol \\alpha}_s^{(n)})$ (while keeping the other variables unchanged), we can easily verify that the updated variables ${\\boldsymbol \\alpha}^{(n+1)}$ remains in the hyper-plane  $ \\mathbf{y}^\\intercal {\\boldsymbol \\alpha} = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwtBfwzhRtLV"
   },
   "source": [
    "Next, we will compute the maximum step size $\\eta_n$ that is allowed along the projected gradient $\\tilde{\\nabla} L({\\boldsymbol \\alpha}_s^{(n)})$ to ensure the updated ${\\boldsymbol \\alpha}^{(n+1)}$ still satisfies the box constraint $[0,1]$. For each variable $\\alpha_k \\in {\\boldsymbol \\alpha}_s$, the box bound for the current update depends on the sign of its corresponding projected gradient $\\tilde{\\nabla} L( \\alpha_k^{(n)})$:\n",
    "$$b_k  =  \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "0  & \\mbox{if } \\tilde{\\nabla} L( \\alpha_k^{(n)}) >0  \\\\\n",
    "C & \\mbox{if } \\tilde{\\nabla} L( \\alpha_k^{(n)})  <0  \n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "Thus, the maximum allowed step size is computed as:\n",
    "$$\n",
    "\\eta_n  = \\min_{\\alpha_k \\in {\\boldsymbol \\alpha}_s} \\frac{  \\left|\\alpha_k^{(n)} - b_k \\right|}{ \\left|\\tilde{\\nabla} L( \\alpha_k^{(n)}) \\right| + \\epsilon}\n",
    "$$\n",
    "where $\\epsilon=10^{-3}$ is added for numerical stability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4nqfQ0xinwC"
   },
   "outputs": [],
   "source": [
    "# solve linear SVMs using projected gradient descent (PGD)\n",
    "import numpy as np\n",
    "\n",
    "class mySVM1():\n",
    "  def __init__(self, kernel='linear', optimizer='pgd', debug=0, threshold=0.001, \\\n",
    "               lr=1.0, max_epochs=10, batch_size=2, C=1):\n",
    "    self.kernel = kernel        # kernel type\n",
    "    self.optimizer = optimizer  # which optimizer is used to solve quadratic programming\n",
    "    self.lr = lr                # max learning rate in PGD\n",
    "    self.max_epochs = max_epochs   # max epochs in PGD\n",
    "    self.batch_size = batch_size   # size of each subset in PGD\n",
    "    self.debug = debug             # whether print debugging info\n",
    "    self.threshold = threshold     # threshold to filter out support vectors \n",
    "\n",
    "    self.C = C     # C for the soft-margin term\n",
    "\n",
    "  # Linear Kernel Function\n",
    "  # X[N,d]: training samples;  Y[M,d]: other training samples\n",
    "  # return Q[N,N]: linear kernel matrix between X and Y\n",
    "  def Kernel(self, X, Y):\n",
    "    if (self.kernel == 'linear'):\n",
    "      K = X @ Y.T\n",
    "\n",
    "    return K\n",
    "\n",
    "  # construct matrix Q from any kernel function for dual SVM optimization \n",
    "  def QuadraticMatrix(self, X, y):\n",
    "    Q = np.outer(y, y) * self.Kernel(X, X) \n",
    "    return Q\n",
    "\n",
    "  # use projected gradient descent to solve quadratic program \n",
    "  # refer to Algorithm 6.5 on page 127\n",
    "  # Q[N,N]: quadratic matrix;  y[N]: training labels (+1 or -1)\n",
    "  def PGD(self, Q, y):\n",
    "    N = Q.shape[0]   # num of training samples\n",
    "    alpha = np.zeros(N)\n",
    "    prev_L = 0.0\n",
    "\n",
    "    for epoch in range(self.max_epochs):\n",
    "      indices = np.random.permutation(N)  #randomly shuffle data indices\n",
    "      for batch_start in range(0, N, self.batch_size):\n",
    "        idx = indices[batch_start:batch_start + self.batch_size] # indices of the selected subset\n",
    "        alpha_s = alpha[idx]\n",
    "        y_s = y[idx]\n",
    "\n",
    "        grad_s = Q[idx,:] @ alpha - np.ones(idx.shape[0])\n",
    "        proj_grad_s = grad_s - np.dot(y_s,grad_s)/np.dot(y_s, y_s)*y_s\n",
    "\n",
    "        bound = np.zeros(idx.shape[0])\n",
    "        bound[proj_grad_s < 0] = self.C \n",
    "\n",
    "        eta = np.min(np.abs(alpha_s-bound)/(np.abs(proj_grad_s)+0.001))\n",
    "\n",
    "        alpha[idx] -= min(eta, self.lr) * proj_grad_s\n",
    "\n",
    "      L = 0.5 * alpha.T @ Q @ alpha - np.sum(alpha) # objectibve function \n",
    "      if (L > prev_L): \n",
    "        if (self.debug>0):\n",
    "          print('Early stopping at epoch={epoch}!')\n",
    "        break\n",
    "      \n",
    "      if (self.debug>1):\n",
    "        print(f'[PGD optimizer] epoch = {epoch}: L = {L:.5f}  (# of support vectors = {(alpha>self.threshold).sum()})')\n",
    "        print(f'                 alpha: max={np.max(alpha)} min={np.min(alpha)} orthogonal constraint={np.dot(alpha,y):.2f}')\n",
    "\n",
    "      prev_L = L\n",
    "\n",
    "    return alpha\n",
    "\n",
    "  # train SVM from training samples\n",
    "  # X[N,d]: input features;  y[N]: output labels (+1 or -1)\n",
    "  def fit(self, X, y):\n",
    "    if(self.kernel != 'linear'):\n",
    "      print(\"Error: only linear kernel is supported!\")\n",
    "      return\n",
    "\n",
    "    Q = self.QuadraticMatrix(X, y)\n",
    "\n",
    "    alpha = self.PGD(Q, y)\n",
    "\n",
    "    #save support vectors (pruning all data with alpha==0)\n",
    "    self.X_SVs = X[alpha>self.threshold]\n",
    "    self.y_SVs = y[alpha>self.threshold]\n",
    "    self.alpha_SVs = alpha[alpha>self.threshold]\n",
    "\n",
    "    # compute weight vector for linear SVMs (refer to the formula on page 120)\n",
    "    if(self.kernel == 'linear'):\n",
    "      self.w = (self.y_SVs * self.alpha_SVs) @ self.X_SVs\n",
    "\n",
    "    # estimate b\n",
    "    idx = np.nonzero(np.logical_and(self.alpha_SVs>self.threshold,self.alpha_SVs<self.C-self.threshold))\n",
    "    if(len(idx) == 0):\n",
    "      idx = np.nonzero(self.alpha_SVs>self.threshold)\n",
    "    # refer to the formula on page 125 (above Figure 6.11) \n",
    "    b = self.y_SVs[idx] - (self.y_SVs * self.alpha_SVs) @ self.Kernel(self.X_SVs, self.X_SVs[idx])\n",
    "    self.b = np.median(b)\n",
    "    \n",
    "    return \n",
    "\n",
    "  # use SVM from prediction\n",
    "  # X[N,d]: input features\n",
    "  def predict(self, X):\n",
    "    if(self.kernel != 'linear'):\n",
    "      print(\"Error: only linear kernel is supported!\")\n",
    "      return \n",
    "\n",
    "    y = X @ self.w + self.b \n",
    "\n",
    "    return np.sign(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SlSJMUUa8uIw",
    "outputId": "2b293c21-d942-44c7-d2e5-238b2e80ad3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MY linear SVM (C=0.1): training accuracy=96.53%  test accuracy=96.82%\n",
      "MY linear SVM (C=1): training accuracy=97.10%  test accuracy=96.67%\n",
      "MY linear SVM (C=2): training accuracy=96.96%  test accuracy=96.98%\n",
      "MY linear SVM (C=4): training accuracy=96.85%  test accuracy=96.67%\n",
      "MY linear SVM (C=10): training accuracy=96.35%  test accuracy=96.47%\n"
     ]
    }
   ],
   "source": [
    "for c in [0.1, 1, 2, 4, 10]:\n",
    "  svm = mySVM1(max_epochs=10, lr=2.0, C=c, kernel='linear')\n",
    "  svm.fit(X_train,y_train)\n",
    "\n",
    "  predict = svm.predict(X_train)\n",
    "  train_acc = np.count_nonzero(np.equal(predict,y_train))/y_train.size\n",
    "  predict = svm.predict(X_test)\n",
    "  test_acc = np.count_nonzero(np.equal(predict,y_test))/y_test.size\n",
    "  print(f'MY linear SVM (C={c}): training accuracy={100*train_acc:.2f}%  test accuracy={100*test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arIqubRpsg73"
   },
   "source": [
    "When we compare the above results with those generated by SVC from *sciki-learn*, we can see that our linear SVM implementation using PGD has achieved comparable performance to *sciki-learn*'s. For example, when we compare test classification accuracy, we can see that  our implementation obtains 96.98% at C=2 while scikit-learn's SVC gets 97.08% at C=10.\n",
    "(Note: the results vary slightly between different runs due to the randomness in the PGD optimization algorithm.)\n",
    "\n",
    "Next, we compare the training time between our PGD implementation and sciki-learn's SVC. From the following results, we can see that their running times are pretty close as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3DGBdXbOwuzW",
    "outputId": "c68c555f-5d01-4362-aebd-3264d5026a6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.5 s, sys: 21.6 ms, total: 10.5 s\n",
      "Wall time: 10.5 s\n",
      "CPU times: user 12.2 s, sys: 1.12 s, total: 13.3 s\n",
      "Wall time: 8.47 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "c=1\n",
    "\n",
    "linearSVM = SVC(kernel='linear', C=c)\n",
    "%timeit linearSVM.fit(X_train,y_train)\n",
    "\n",
    "svm = mySVM1(max_epochs=10, lr=2.0, C=c, kernel='linear')\n",
    "%timeit svm.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZprYPtRETk8q"
   },
   "source": [
    "## **III. Nonlinear SVMs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "928wiSP9uwPj"
   },
   "source": [
    "### **Example 4.3:**\n",
    "\n",
    "*Add two more kernel functions (i.e. polynomial and Gaussian RBF kernels) to extend the above SVM implementation in Example 4.2 for nonlinear SVMs.  Use your nonlinear SVM implementation to build a binary classifier to classify two digits ('3' and '8') in the MNIST data set. Compare your own implementation with that of scikit-learn for these two nonlinear kernel functions in terms of classification accuracy and running speed.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgzAf0u6CBry"
   },
   "source": [
    "First of all, let us consider how to use vectorization to compute various kernel matrices for two sets of feature vectors, i.e. $\\{ \\mathbf{x}_1 , \\cdots, \\mathbf{x}_N\\} $ and $\\{ \\mathbf{y}_1 , \\cdots, \\mathbf{y}_M\\} $,  where $\\mathbf{x}_i \\in \\mathbb{R}^d$ and $\\mathbf{y}_j \\in \\mathbb{R}^d$. \n",
    "\n",
    "As the way on page 112, if we pack all feature vectors $\\mathbf{x}_i$ from the first set row by row as a matrix $\\mathbf{X}  \\in \\mathbb{R}^{N \\times d}$, and all feature vectors $\\mathbf{y}_j$ from the second set  row by row as a matrix $\\mathbf{Y}  \\in \\mathbb{R}^{M \\times d}$, we can conveniently compute the kernel matrices for different kernel functions as follows:\n",
    "\n",
    "\n",
    "(1) Linear kernel $\\Phi(\\textbf{x}_i,\\textbf{y}_j) = \\textbf{x}^\\intercal_i \\textbf{y}_j $: \n",
    "\n",
    "$$\n",
    "\\mathbf{K} = \\Bigg[ \\Phi(\\textbf{x}_i,\\textbf{y}_j) \\Bigg]_{N \\times M}\n",
    "= \\begin{bmatrix}  \n",
    "   \\textbf{x}_1^\\intercal \\textbf{y}_1    &  \\cdots  & \\textbf{x}_1^\\intercal \\textbf{y}_M  \\\\\n",
    "   \\vdots   & \\textbf{x}_i^\\intercal \\textbf{y}_j  &  \\vdots \\\\\n",
    "   \\textbf{x}_N^\\intercal \\textbf{y}_1  &  \\cdots   & \\textbf{x}_N^\\intercal \\textbf{y}_M \n",
    " \\end{bmatrix}_{N \\times M} \n",
    " = \\mathbf{X} \\mathbf{Y}^\\intercal\n",
    "$$\n",
    "\n",
    "(2)  Polynomial kernel $\\Phi(\\textbf{x}_i,\\textbf{y}_j) = (\\textbf{x}^\\intercal_i \\textbf{y}_j+1)^p $:\n",
    "\n",
    "$$\n",
    "\\mathbf{K} = \\Bigg[ \\Phi(\\textbf{x}_i,\\textbf{y}_j) \\Bigg]_{N \\times M}\n",
    "= \\begin{bmatrix}  \n",
    "   (\\textbf{x}_1^\\intercal \\textbf{y}_1+1)^p    &  \\cdots  & (\\textbf{x}_1^\\intercal \\textbf{y}_M+1)^p \\\\\n",
    "   \\vdots   & (\\textbf{x}_i^\\intercal \\textbf{y}_j+1)^p  &  \\vdots \\\\\n",
    "   (\\textbf{x}_N^\\intercal \\textbf{y}_1+1)^p  &  \\cdots   & (\\textbf{x}_N^\\intercal \\textbf{y}_M +1)^p\n",
    " \\end{bmatrix}_{N \\times M} \n",
    " = \\mbox{power} \\Big( \\mathbf{X} \\mathbf{Y}^\\intercal +1, p)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G643QnQYVCNF"
   },
   "source": [
    "(3)  Gaussian RBF kernel $\\Phi(\\textbf{x}_i,\\textbf{y}_j) = \\exp(-\\gamma ||\\textbf{x}_i - \\textbf{y}_j||^2)$: \n",
    "\n",
    "We can show that \n",
    "$$\n",
    "||\\textbf{x}_i - \\textbf{y}_j||^2 = \\big( \\textbf{x}_i - \\textbf{y}_j \\big)^\\intercal \\big( \\textbf{x}_i - \\textbf{y}_j \\big)\n",
    "= \\textbf{x}_i^\\intercal \\textbf{x}_i + \\textbf{y}_j^\\intercal \\textbf{y}_j - 2 \\textbf{x}_i^\\intercal \\textbf{y}_j\n",
    "$$\n",
    "\n",
    "We first compute two diagonal vectors as follows:\n",
    "$$\n",
    "\\mathbf{a} = \n",
    "\\begin{bmatrix} \n",
    "\\textbf{x}_1^\\intercal \\textbf{x}_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\textbf{x}_N^\\intercal \\textbf{x}_N\n",
    "\\end{bmatrix}_{N \\times 1} \n",
    "= \\mbox{diag} \\big( \\mathbf{X} \\mathbf{X}^\\intercal \\big)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{b} = \n",
    "\\begin{bmatrix} \n",
    "\\textbf{y}_1^\\intercal \\textbf{y}_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\textbf{y}_N^\\intercal \\textbf{y}_N\n",
    "\\end{bmatrix}_{M \\times 1} \n",
    "= \\mbox{diag} \\big( \\mathbf{Y} \\mathbf{Y}^\\intercal \\big)\n",
    "$$\n",
    "\n",
    "Finally, we can verify that\n",
    "\\begin{eqnarray}\n",
    "\\mathbf{K} & = &  \\Bigg[ \\Phi(\\textbf{x}_i,\\textbf{y}_j) \\Bigg]_{N \\times M} \n",
    " =   \\begin{bmatrix}  \n",
    "   \\exp(-\\gamma ||\\textbf{x}_1 - \\textbf{y}_1||^2)    &  \\cdots  & \\exp(-\\gamma ||\\textbf{x}_1 - \\textbf{y}_M||^2) \\\\\n",
    "   \\vdots   &  \\exp(-\\gamma ||\\textbf{x}_i - \\textbf{y}_i||^2)  &  \\vdots \\\\\n",
    "    \\exp(-\\gamma ||\\textbf{x}_N - \\textbf{y}_1||^2) &  \\cdots   &  \\exp(-\\gamma ||\\textbf{x}_N - \\textbf{y}_M||^2)\n",
    " \\end{bmatrix}_{N \\times M} \n",
    " \\nonumber \\\\\n",
    " & = & \n",
    " \\exp\\Big( -\\gamma \\big( \\mathbf{a} \\, \\mathbf{1}_{\\tiny M}^\\intercal  +  \\mathbf{1}_{\\tiny N} \\, \\mathbf{b}^\\intercal - 2\\, \\mathbf{X} \\mathbf{Y}^\\intercal \\big) \\Big)\n",
    "  \\nonumber \n",
    "\\end{eqnarray}\n",
    "where $\\mathbf{1}_m$ denotes an $m$-dimension vector consisting of all 1's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tT8u3o-dJkuI"
   },
   "outputs": [],
   "source": [
    "# extend for nonlinear SVMs by adding polynomial and RBF kernel functions \n",
    "#\n",
    "import numpy as np\n",
    "\n",
    "class mySVM2():\n",
    "  def __init__(self, kernel='linear', optimizer='pgd', debug=0, threshold=0.001, \\\n",
    "               lr=1.0, max_epochs=20, batch_size=2, C=1, order=3, gamma=1.0):\n",
    "    self.kernel = kernel           # kernel type\n",
    "    self.optimizer = optimizer     # which optimizer is used to solve quadratic programming\n",
    "    self.lr = lr                   # max learning rate in PGD\n",
    "    self.max_epochs = max_epochs   # max epochs in PGD\n",
    "    self.batch_size = batch_size   # size of each subset in PGD\n",
    "    self.debug = debug             # whether print debugging info\n",
    "    self.threshold = threshold     # threshold to filter out support vectors \n",
    "\n",
    "    self.C = C                     # C for the soft-margin term\n",
    "    self.order = order             # power order for polynomial kernel\n",
    "    self.gamma = gamma             # gamma for Gaussian RBF kernel\n",
    "\n",
    "  # Kernel Function\n",
    "  # X[N,d]: training samples;  Y[M,d]: other training samples\n",
    "  # return Q[N,N]: linear kernel matrix between X and Y\n",
    "  def Kernel(self, X, Y):\n",
    "    if (self.kernel == 'linear'):\n",
    "      K = X @ Y.T\n",
    "    elif (self.kernel == 'poly'):\n",
    "      K = np.power(X @ Y.T +1, self.order)\n",
    "    elif (self.kernel == 'rbf'): \n",
    "      d1 = np.sum(X*X, axis=1) \n",
    "      d2 = np.sum(Y*Y, axis=1)\n",
    "      K = np.outer(d1, np.ones(Y.shape[0])) + np.outer(np.ones(X.shape[0]), d2) \\\n",
    "          - 2 * X @ Y.T\n",
    "      K = np.exp(-self.gamma * K) \n",
    "      \n",
    "    return K\n",
    "\n",
    "  # construct matrix Q from any kernel function for dual SVM optimization \n",
    "  def QuadraticMatrix(self, X, y):\n",
    "    Q = np.outer(y, y) * self.Kernel(X, X) \n",
    "    return Q\n",
    "\n",
    "  # use projected gradient descent to solve quadratic program \n",
    "  # refer to Algorithm 6.5 on page 127\n",
    "  # Q[N,N]: quadratic matrix;  y[N]: training labels (+1 or -1)\n",
    "  def PGD(self, Q, y):\n",
    "    N = Q.shape[0]   # num of training samples\n",
    "    alpha = np.zeros(N)\n",
    "    prev_L = 0.0\n",
    "\n",
    "    for epoch in range(self.max_epochs):\n",
    "      indices = np.random.permutation(N)  #randomly shuffle data indices\n",
    "      for batch_start in range(0, N, self.batch_size):\n",
    "        idx = indices[batch_start:batch_start + self.batch_size] # indices of the current subset\n",
    "        alpha_s = alpha[idx]\n",
    "        y_s = y[idx]\n",
    "\n",
    "        grad_s = Q[idx,:] @ alpha - np.ones(idx.shape[0])\n",
    "        proj_grad_s = grad_s - np.dot(y_s,grad_s)/np.dot(y_s, y_s)*y_s\n",
    "\n",
    "        bound = np.zeros(idx.shape[0])\n",
    "        bound[proj_grad_s < 0] = self.C \n",
    "\n",
    "        eta = np.min(np.abs(alpha_s-bound)/(np.abs(proj_grad_s)+0.001))\n",
    "\n",
    "        alpha[idx] -= min(eta, self.lr) * proj_grad_s\n",
    "\n",
    "      L = 0.5 * alpha.T @ Q @ alpha - np.sum(alpha) # objectibve function \n",
    "      if (L > prev_L): \n",
    "        if (self.debug>0):\n",
    "          print(f'Early stopping at epoch={epoch}! (reduce learning rate lr)')\n",
    "        break\n",
    "      \n",
    "      if (self.debug>1):\n",
    "        print(f'[PGD optimizer] epoch = {epoch}: L = {L:.5f}  (# of support vectors = {(alpha>self.threshold).sum()})')\n",
    "        print(f'                 alpha: max={np.max(alpha)} min={np.min(alpha)} orthogonal constraint={np.dot(alpha,y):.2f}')\n",
    "\n",
    "      prev_L = L\n",
    "\n",
    "    return alpha\n",
    "\n",
    "  # train SVM from training samples\n",
    "  # X[N,d]: input features;  y[N]: output labels (+1 or -1)\n",
    "  def fit(self, X, y):\n",
    "    if(self.kernel != 'linear' and self.kernel != 'poly' and self.kernel != 'rbf'):\n",
    "      print(\"Error: only linear/poly/rbf kernel is supported!\")\n",
    "      return\n",
    "\n",
    "    Q = self.QuadraticMatrix(X, y)\n",
    "\n",
    "    alpha = self.PGD(Q, y)\n",
    "\n",
    "    #save support vectors (pruning all data with alpha==0)\n",
    "    self.X_SVs = X[alpha>self.threshold]\n",
    "    self.y_SVs = y[alpha>self.threshold]\n",
    "    self.alpha_SVs = alpha[alpha>self.threshold]\n",
    "\n",
    "    if(self.kernel == 'linear'):\n",
    "      self.w = (self.y_SVs * self.alpha_SVs) @ self.X_SVs\n",
    "\n",
    "    # estimate b\n",
    "    idx = np.nonzero(np.logical_and(self.alpha_SVs>self.threshold,self.alpha_SVs<self.C-self.threshold))\n",
    "    if(len(idx) == 0):\n",
    "      idx = np.nonzero(self.alpha_SVs>self.threshold)\n",
    "    # refer to the formula on page 125 (above Figure 6.11) \n",
    "    b = self.y_SVs[idx] - (self.y_SVs * self.alpha_SVs) @ self.Kernel(self.X_SVs, self.X_SVs[idx])\n",
    "    self.b = np.median(b)\n",
    "    \n",
    "    return \n",
    "\n",
    "  # use SVM from prediction\n",
    "  # X[N,d]: input features\n",
    "  def predict(self, X):\n",
    "    if(self.kernel != 'linear' and self.kernel != 'poly' and self.kernel != 'rbf'):\n",
    "      print(\"Error: only linear/poly/rbf kernel is supported!\")\n",
    "      return\n",
    "\n",
    "    if(self.kernel == 'linear'):\n",
    "      y = X @ self.w + self.b \n",
    "    else:\n",
    "      y = (self.y_SVs * self.alpha_SVs) @ self.Kernel(self.X_SVs, X) + self.b \n",
    "\n",
    "    return np.sign(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bTbusvyYPDFn",
    "outputId": "20bc42e6-b003-4a75-9c92-021470335620"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MY linear SVM (C=2): training accuracy=97.31%  test accuracy=96.98%\n"
     ]
    }
   ],
   "source": [
    "c = 2\n",
    "svm = mySVM2(max_epochs=20, lr=1.0, C=c, kernel='linear', debug=0)\n",
    "svm.fit(X_train,y_train)\n",
    "\n",
    "predict = svm.predict(X_train)\n",
    "train_acc = np.count_nonzero(np.equal(predict,y_train))/y_train.size\n",
    "predict = svm.predict(X_test)\n",
    "test_acc = np.count_nonzero(np.equal(predict,y_test))/y_test.size\n",
    "print(f'MY linear SVM (C={c}): training accuracy={100*train_acc:.2f}%  test accuracy={100*test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BOJg45BNVxQf",
    "outputId": "41d372d8-7180-47e4-d156-ab5ae71b4bdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MY poly SVM (C=2, d=3): training accuracy=99.77%  test accuracy=99.50%\n"
     ]
    }
   ],
   "source": [
    "c = 2\n",
    "d = 3 \n",
    "svm = mySVM2(max_epochs=20, lr=0.1, C=c, kernel='poly', order=d, debug=0)\n",
    "svm.fit(X_train,y_train)\n",
    "\n",
    "predict = svm.predict(X_train)\n",
    "train_acc = np.count_nonzero(np.equal(predict,y_train))/y_train.size\n",
    "predict = svm.predict(X_test)\n",
    "test_acc = np.count_nonzero(np.equal(predict,y_test))/y_test.size\n",
    "print(f'MY poly SVM (C={c}, d={d}): training accuracy={100*train_acc:.2f}%  test accuracy={100*test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7VxwMuDnZbD5",
    "outputId": "c28dd151-b419-4cf0-a502-bd19209eaa3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MY RBF SVM (C=2, gamma=2.0): training accuracy=99.97%  test accuracy=99.60%\n"
     ]
    }
   ],
   "source": [
    "c = 2\n",
    "g = 2.0\n",
    "svm = mySVM2(max_epochs=20, lr=1.0, C=c, kernel='rbf', gamma=g, debug=0)\n",
    "svm.fit(X_train,y_train)\n",
    "\n",
    "predict = svm.predict(X_train)\n",
    "train_acc = np.count_nonzero(np.equal(predict,y_train))/y_train.size\n",
    "predict = svm.predict(X_test)\n",
    "test_acc = np.count_nonzero(np.equal(predict,y_test))/y_test.size\n",
    "print(f'MY RBF SVM (C={c}, gamma={g}): training accuracy={100*train_acc:.2f}%  test accuracy={100*test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tSTFZDqREFx"
   },
   "source": [
    "From the above results, we can see that our SVM implmentation delivers comparable classification accuracies with the scikit-learn SVC functions for all three kernel functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-5Q36KVTtvB"
   },
   "source": [
    "## **Exercises**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1OX8BeWRiJG"
   },
   "source": [
    "### **Problem 4.1:**\n",
    "\n",
    "Use the *one-versus-one* strategy discussed in Section 6.5.5 (page 127) to extend the above binary SVMs to deal with a pattern classification task involving  any number of classes. Use your extension to build a 10-class classifier to recognize all 10 digits in the MNIST data set. Compare three different kernels and fine-tune their hyper-parameters towards the best possible accuracy in each case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mICmXccJTFYs"
   },
   "source": [
    "### **Problem 4.2:**\n",
    "\n",
    "Refer to Q6.12 (page 131), derive the closed-form solution to update any two varaibles in $\\boldsymbol{\\alpha}$ if we keep all other variables in $\\boldsymbol{\\alpha}$ constant in the quadratic programming problem (SVM4 on page 122). Based on this result, implement the famous *sequential minimization optimization* (SMO) method as another optimizer option for our SVM implementation (besides PGD).  Compare  SMO with PGD in terms of their convergence speeds and final classification accuracies. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "C-5Q36KVTtvB"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
