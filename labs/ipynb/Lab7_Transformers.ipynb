{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72pszDXYGC7L"
   },
   "source": [
    "# **Lab 7: Transformers**\n",
    "\n",
    "---\n",
    "NOTE: This is a lab project accompanying the following book [MLF] and it should be used together with the book.\n",
    "\n",
    "[MLF] *H. Jiang*, \"[Machine Learning Fundamentals: A Concise Introduction](http://wiki.eecs.yorku.ca/user/hj/research:mlfbook)\", Cambridge University Press, 2021.  ([bibtex](http://www.cse.yorku.ca/~hj/mlf-jiang.bib))\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWg6ZuV9GVR6"
   },
   "source": [
    "The purpose of this lab is to study the popular transformer achitecture for a simple language modeling task. First, we show how to implement the multi-head attention module in transformers using *pytorch*. Next, we extend it to a multi-layer transformer structure for general sequence modeling purpose. In particular, we use the multi-layer transformer model to perform char-level language modeling on a small text corpus, i.e. the *tiny-shakespeare* corpus. At last, after the transformer is trained, we how that it can be used to generate new text similar to the training samples. \n",
    "\n",
    "Prerequisites: basic understanding on *pytorch*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuxeXFn3JqZH"
   },
   "source": [
    "## **I. Multi-head Self-Attention Module**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6znEB6VJJ16c"
   },
   "source": [
    "### **Example 7.1:**\n",
    "\n",
    "*Use pytorch to implement a multi-head causal self-attention model similar to Figure 8.27 on page 173. The causal attention means that each output vector depends on only all preceding input vectors but not any vectors appearing after it.  Compare the running speeds of its forward and backward passes when running on CPUs and GPUs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QykZSvtLDnn"
   },
   "source": [
    "All hyper-parameters: (usually we have $d= n \\times h$) \n",
    "*  $d$: embedding dimension \n",
    "*  $B$: batch size\n",
    "*  $T$: block size\n",
    "*  $n$: head number \n",
    "*  $h$: head size\n",
    "\n",
    "\n",
    "1. Given a mini-batch of $B$ input sequences, packed as $\\mathbf{X} \\in \\mathbb{R}^{B \\times d \\times T}$, and a multi-head transformer consisting of all parameter matrices as $\\mathbf{A}^{(j)}, \\mathbf{B}^{(j)}, \\mathbf{C}^{(j)} \\in \\mathbb{R}^{h \\times d}$   ($j=1,2,\\cdots,n$). We first arrange these matrices into three larger parameter matrices,\n",
    "$\\mathbf{A}, \\mathbf{B}, \\mathbf{C} \\in \\mathbb{R}^{nh \\times d}$, and \n",
    "then further pack these three matrices into a single large matrix  $\\mathbf{M} \\in \\mathbb{R}^{3nh \\times d}$.\n",
    "We can generate query, key and value matrices as follows:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{Q} \\\\\n",
    "\\mathbf{K} \\\\\n",
    "\\mathbf{V} \\end{bmatrix}  = \n",
    "\\begin{bmatrix}\\mathbf{A}\\\\\n",
    " \\mathbf{B} \\\\\\mathbf{C} \n",
    "\\end{bmatrix}  \\mathbf{X}\n",
    "= \\mathbf{M} \\, \\mathbf{X}\n",
    "$$\n",
    "where $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\in \\mathbb{R}^{B \\times nh \\times T}$.\n",
    "\n",
    "2.   Re-shape the query, key and value matrices as follows: \n",
    "$$\n",
    "\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}: \\mathbb{R}^{B \\times nh \\times T}  → \\mathbb{R}^{B \\times n \\times h \\times T}\n",
    "$$\n",
    "\n",
    "3. Apply the point-wise attention as: \n",
    "$$\n",
    "\\mathbf{Q}^\\intercal \\mathbf{K} \\in \\mathbb{R}^{B \\times n \\times T \\times T}\n",
    "$$\n",
    "If necessary, apply an **upper** triangular masking for causal attetion. \n",
    "\n",
    "4. Apply softmax **column by column** \n",
    "$$\n",
    "\\mathcal{A} =  \\mathrm{softmax} \\Big(\\mathbf{Q}^\\intercal \\mathbf{K} \\Big)\n",
    "\\;\\;\\; (\\in \\mathbb{R}^{B \\times n \\times T \\times T})\n",
    "$$\n",
    "\n",
    "\n",
    "5. Generate the output\n",
    "$$\n",
    "\\mathbf{Z} = \\mathbf{V} \\,  \\mathcal{A}   \\;\\;\\; (\\in \\mathbb{R}^{B \\times n \\times h \\times T})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vrS05MUpF4sk"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class config():\n",
    "  def __init__(self, batch_size = 10, n_embd = 768, block_size = 128, n_head = 8, causal = True, device='cuda'):\n",
    "    assert n_embd %  n_head == 0\n",
    "    self.batch_size = batch_size\n",
    "    self.n_embd = n_embd\n",
    "    self.block_size = block_size\n",
    "    self.n_head = n_head \n",
    "    self.causal = causal\n",
    "    self.device = device\n",
    "\n",
    "class SelfAttentionModule(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.batch_size = cfg.batch_size\n",
    "        self.n_embd = cfg.n_embd\n",
    "        self.block_size = cfg.block_size\n",
    "        self.n_head = cfg.n_head\n",
    "        self.device = cfg.device\n",
    "        self.causual = cfg.causal\n",
    "        mask = torch.tril(torch.ones(self.block_size, self.block_size)).transpose(0,1).view(1, 1, self.block_size, self.block_size)\n",
    "        mask = mask.to(self.device)\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, X, M):\n",
    "        B, d, T = X.size() # batch size,  embedding dimensionality (n_embd), block size\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        Q, K, V  = (M @ X).split(d, dim=1)\n",
    "        K = K.view(B, self.n_head, d // self.n_head, T)   # (B, nh, hs, T)\n",
    "        Q = Q.view(B, self.n_head, d // self.n_head, T)   # (B, nh, hs, T)\n",
    "        V = V.view(B, self.n_head, d // self.n_head, T)   # (B, nh, hs, T)\n",
    "\n",
    "        att = (Q.transpose(-2, -1) @ K) * (1.0 / math.sqrt(Q.size(-2)))\n",
    "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-2)   # column-wise softmax\n",
    "        Z = V @ att   # (B, nh, hs, T) x (B, nh, T, T) -> (B, nh, hs, T)\n",
    "\n",
    "        Z = Z.contiguous().view(B, d, T)  # re-assemble all head outputs side by side\n",
    "\n",
    "        return Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RHNFuyqONLVK",
    "outputId": "18db7012-ff93-411f-af72-20638237ca88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size=10 n_embd=768 block_size=128 n_head=8 device=cpu\n",
      "pytorch forward pass loss: 19929.55859375\n",
      "forward() pass and pytorch auto-grad backward() running on cpu:\n",
      "64.5 ms ± 12 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "113 ms ± 11.1 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "batch_size=10 n_embd=768 block_size=128 n_head=8 device=cuda\n",
      "pytorch forward pass loss: 19929.556640625\n",
      "forward() pass and pytorch auto-grad backward() running on cuda:\n",
      "2.08 ms ± 5.13 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "9.29 ms ± 271 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "cfg=config()\n",
    "\n",
    "def loss(model, x, M):\n",
    "  y = model(x,M)\n",
    "  return torch.sum(y*y)\n",
    "\n",
    "X = torch.rand((cfg.batch_size, cfg.n_embd, cfg.block_size), requires_grad=True)\n",
    "M = (torch.rand((3*cfg.n_embd, cfg.n_embd),requires_grad=True) - 0.5) * (1.0/math.sqrt(cfg.n_embd)) \n",
    "\n",
    "print(f'batch_size={cfg.batch_size} n_embd={cfg.n_embd} block_size={cfg.block_size} n_head={cfg.n_head} device={cfg.device}')\n",
    "\n",
    "X = X.to(cfg.device)\n",
    "M = M.to(cfg.device)\n",
    "\n",
    "mm1=SelfAttentionModule(cfg)\n",
    "\n",
    "ls = loss(mm1, X, M)\n",
    "print(f'pytorch forward pass loss: {ls}')\n",
    "\n",
    "print(f'forward() pass and pytorch auto-grad backward() running on {cfg.device}:')\n",
    "%timeit loss(mm1, X, M)\n",
    "%timeit ls.backward(retain_graph=True)\n",
    "\n",
    "#######################\n",
    "cfg.device ='cuda'\n",
    "\n",
    "print(f'batch_size={cfg.batch_size} n_embd={cfg.n_embd} block_size={cfg.block_size} n_head={cfg.n_head} device={cfg.device}')\n",
    "\n",
    "X = X.to(cfg.device)\n",
    "M = M.to(cfg.device)\n",
    "\n",
    "mm2=SelfAttentionModule(cfg)\n",
    "\n",
    "ls = loss(mm2, X, M)\n",
    "print(f'pytorch forward pass loss: {ls}')\n",
    "\n",
    "print(f'forward() pass and pytorch auto-grad backward() running on {cfg.device}:')\n",
    "%timeit loss(mm2, X, M)\n",
    "%timeit ls.backward(retain_graph=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdB6QYWsREFK"
   },
   "source": [
    "## **II. Transformers for Language Modelling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1d7JNtngRUt9"
   },
   "source": [
    "### **Example 7.2:**\n",
    "\n",
    "*Use pytorch to implement a multi-layer causal transformer model on page 174, similar to the popular GPT architecture, for char-level language modelling on the tiny-shakespeare corpus.\n",
    "In other words, each char in text is treated as a distinct token in the language model. The transformer is trained to predict next character based on all preceding characters in the transformer input block. After it is trained, use it to generate new text sequences.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Es3_2TwNRTxo",
    "outputId": "5fde555f-94b8-4de9-abe8-ed9d756876b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1Sl0000nPuCW6RcF3b3SL2n9EPJpI582q input.txt\n",
      "Building directory structure completed\n"
     ]
    }
   ],
   "source": [
    "# download tiny shakespeare text corpus from Google drive\n",
    "\n",
    "!gdown --folder https://drive.google.com/drive/folders/1VtY71Iym2uOC4bUUukJ9I-b1JgR5e18o 2> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31WtnUCvUAWr",
    "outputId": "ae75fcf1-28c6-4e43-ccb6-7167f74cf492"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 1,115,394\n",
      "all the unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 65\n",
      "train has 1,003,854 tokens\n",
      "val has 111,540 tokens\n"
     ]
    }
   ],
   "source": [
    "# load text file as a character string; build char-level vocabulary, encoder and decoder\n",
    "#  (borrowed from nanoGPT, https://github.com/karpathy/nanoGPT)\n",
    "\n",
    "with open('tinyshakespeare/input.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "print(f\"length of dataset in characters: {len(data):,}\")\n",
    "\n",
    "# get all the unique characters that occur in this text\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(\"all the unique characters:\", ''.join(chars))\n",
    "print(f\"vocab size: {vocab_size:,}\")\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# create the train and test splits\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "# encode both to integers\n",
    "train_ids = encode(train_data)\n",
    "val_ids = encode(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Hn3gM5Z-Umh_"
   },
   "outputs": [],
   "source": [
    "# Define all parts in a GPT model, such as attention, MLP, LN modules\n",
    "#  (adapted from nanoGPT, https://github.com/karpathy/nanoGPT)\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class config():\n",
    "  def __init__(self, batch_size=10, n_layer=12, n_head=12, n_embd=768, block_size=1024, vocab_size=50304, causal=True, device='cpu'):\n",
    "    assert n_embd %  n_head == 0\n",
    "    self.batch_size = batch_size\n",
    "    self.n_embd = n_embd\n",
    "    self.block_size = block_size\n",
    "    self.n_head = n_head \n",
    "    self.causal = causal\n",
    "    self.device = device\n",
    "    self.n_layer = n_layer\n",
    "    self.vocab_size = vocab_size\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size)).transpose(0,1)\n",
    "                            .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, C // self.n_head, self.n_head).transpose(1, 3) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, C // self.n_head, self.n_head).transpose(1, 3) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, C // self.n_head, self.n_head).transpose(1, 3) # (B, nh, T, hs)\n",
    "\n",
    "        att = (q.transpose(-2, -1) @ k) * (1.0 / math.sqrt(q.size(-2)))\n",
    "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-2)\n",
    "        y = v @ att  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 3).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm([config.n_embd])\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm([config.n_embd])\n",
    "\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "      \n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm([config.n_embd]),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # with weight tying when using torch.compile() some warnings get generated:\n",
    "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        # Return the number of parameters in the model.\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71YeBTSgVTyS",
    "outputId": "f6b78558-dae6-4fe8-c6ae-7be42470dad6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing a new model from scratch\n",
      "number of parameters: 21.29M\n",
      "step 0: train loss 6.0489, val loss 6.1314 (time lapses 0.0444 seconds)\n",
      "step 1000: train loss 1.9380, val loss 2.0659 (time lapses 74.5444 seconds)\n",
      "step 2000: train loss 1.7263, val loss 1.8830 (time lapses 73.8705 seconds)\n",
      "step 3000: train loss 1.6312, val loss 1.8060 (time lapses 74.0807 seconds)\n",
      "step 4000: train loss 1.5833, val loss 1.7804 (time lapses 73.9943 seconds)\n",
      "step 5000: train loss 1.5541, val loss 1.7657 (time lapses 73.8365 seconds)\n",
      "step 6000: train loss 1.5213, val loss 1.7317 (time lapses 73.7452 seconds)\n",
      "step 7000: train loss 1.5082, val loss 1.7253 (time lapses 73.5869 seconds)\n",
      "step 8000: train loss 1.4806, val loss 1.7024 (time lapses 73.7368 seconds)\n",
      "step 9000: train loss 1.4641, val loss 1.6969 (time lapses 73.7174 seconds)\n",
      "step 10000: train loss 1.4558, val loss 1.6763 (time lapses 73.7342 seconds)\n"
     ]
    }
   ],
   "source": [
    "# training script to learn GPT models based on all given hyper-parameters\n",
    "#  (adapted from nanoGPT, https://github.com/karpathy/nanoGPT)\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# default config values \n",
    "# I/O\n",
    "eval_interval = 1000\n",
    "eval_iters = 200\n",
    "\n",
    "batch_size = 12 \n",
    "block_size = 128 #1024\n",
    "\n",
    "# model\n",
    "n_layer = 3 #12\n",
    "n_head = 8 #12\n",
    "n_embd = 768\n",
    "\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4 # max learning rate\n",
    "max_iters = 10000 # total number of training iterations\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 2000 # how many steps to warm up for\n",
    "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "\n",
    "# system\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = 'bfloat16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "data_tr  = np.array(train_ids, dtype=np.uint16)\n",
    "data_val = np.array(val_ids, dtype=np.uint16)\n",
    "\n",
    "# poor man's data loader\n",
    "def get_batch(split):\n",
    "    data = data_tr if split == 'train' else data_val\n",
    " \n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# init a new model from scratch\n",
    "print(\"Initializing a new model from scratch\")\n",
    "conf = config(batch_size=batch_size, n_layer=n_layer, n_head=n_head, n_embd=n_embd,\\\n",
    "              block_size=block_size, vocab_size=vocab_size, device=device)                                 \n",
    "model = GPT(conf)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(beta1, beta2))\n",
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "\n",
    "# training loop\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "iter_num = 0\n",
    "\n",
    "while True:\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "\n",
    "    logits, loss = model(X, Y)\n",
    "    X, Y = get_batch('train')\n",
    "\n",
    "    # backward pass, with gradient scaling if training in fp16\n",
    "    scaler.scale(loss).backward()\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0: \n",
    "      # timing and logging\n",
    "      t1 = time.time()\n",
    "      dt = t1 - t0\n",
    "      t0 = t1\n",
    "\n",
    "      losses = estimate_loss()\n",
    "      print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f} (time lapses {dt:.4f} seconds)\")  \n",
    " \n",
    "    iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aj-ZBh-cWM15",
    "outputId": "1b3a0137-e99d-4ba6-c7a6-58a12133e911"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Should feasome is thy beother mage-day\n",
      "Beat taunt: many heavy heart.\n",
      "\n",
      "DUKE OF AUMEENE:\n",
      "How and Do now, I must once, to that I coverth,\n",
      "Your besing ensengencellet maid overe, and the next\n",
      "To wanter like him; he the must him sorrow and\n",
      "plaw yeverning I came and gold home:\n",
      "I the good leont not when evices the most revenge\n",
      "For ecst of the house\n",
      "To known bruised sorrow the mufferein.\n",
      "\n",
      "PUCHESS OF POF ELOFFORDON:\n",
      "If I have done, what all his grave him best a\n",
      "are no Rrestime from the w\n",
      "---------------\n",
      "\n",
      "Menening kneess.\n",
      "\n",
      "You cannot:\n",
      "It so, singhion-foot, by wealth.\n",
      "\n",
      "CORIOLANUS:\n",
      "What is the counsel?\n",
      "\n",
      "Messenger:\n",
      "Which might though shouldst not.\n",
      "\n",
      "LUCIO:\n",
      "A perily to medie?\n",
      "\n",
      "LUCIO:\n",
      "For heart too stings he the end, and you'll poove\n",
      "The hearting heard, in our house,\n",
      "Have which frominic was sould you may me,\n",
      "Whose lack to this, my sounder,\n",
      "To I shall power a not with tyranner,\n",
      "How thund thou she certake this road,\n",
      "Peckentuous the sile England, my sharpose\n",
      "Forbs become the time worther for Gloucester's \n",
      "---------------\n",
      "\n",
      "Most any great healthine, which whence not\n",
      "And here did till; I am not were should\n",
      "Is eembles mines an tender infire,\n",
      "For I there, follow not scall bear\n",
      "Hear with I have a cannot do make the dead\n",
      "In to delive attell the leide, twas his laughter woe\n",
      "In amove deter your Heness and lel belove.'\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "How I want Marcius, and brother him to his;\n",
      "The clotter on the the vain hath been seement,\n",
      "Cry the to this new-bable holden her tongue.\n",
      "\n",
      "ANGELO:\n",
      "The perselves that so it it,\n",
      "But I will have \n",
      "---------------\n",
      "\n",
      "The states knows; and plack, say, as I\n",
      "the me of avoides corns to my eteel;\n",
      "Herself, who should teven courselves against brawfull,\n",
      "The tongue the many should will wash a good,\n",
      "Before, my lord morning presentings.\n",
      "\n",
      "FLORIZT:\n",
      "Not, there was all before thy master's cousin?\n",
      "\n",
      "Nurse:\n",
      "God make he upon the wound afollow'd of atter,\n",
      "As it, insuislike he putiness, how that was welk:\n",
      "And stand thou went my head, and sension a brave leave to,\n",
      "The ha particerns witned whom for that I should\n",
      "My the work, and I\n",
      "---------------\n",
      "\n",
      "Thee last old trumphs lost to beting in time:\n",
      "Sorry high thously deadly be and telledge,\n",
      "But her does it soon a world healt.\n",
      "If my liest tongue for thou do the seizens\n",
      "Aft sheek.\n",
      "\n",
      "BRUTKNCARLISBURY:\n",
      "Bay thy cousin' many help his nour presentleman,\n",
      "And nonested some sent the new-eld some sea,\n",
      "But and advisher'd crossing majure of thanks,\n",
      "The seld sleep with be my breaster's danger,\n",
      "And so hold someting biggaretain'd joy,\n",
      "But whas and thinkingbroads the body\n",
      "Merry knows of find still of furthen for\n",
      "---------------\n",
      "\n",
      "\n",
      "MENIUS:\n",
      "A lord, the senatue of trees being in true,\n",
      "Those tatural womanspes we mone not.\n",
      "\n",
      "First Citizen:\n",
      "Even my atters: for thy father, we must thee ride,\n",
      "And sleet for her pasterness of ing one:\n",
      "Till many oftend him with the common sin\n",
      "Of kerns with me issue this bitles shors, here\n",
      "But have to my tantinual name offecting fair!\n",
      "\n",
      "NORTHUMBERLONTH:\n",
      "This Donought promisence on the of insuinal\n",
      "But would thy dreath never late,\n",
      "His crimind my house weaches my father:\n",
      "Nor so, he is son inference, is t\n",
      "---------------\n",
      "\n",
      "Should come outh, for I say, fill not he,\n",
      "I know this revention the king and thought:\n",
      "The susparal: years the arthy villain!\n",
      "\n",
      "Second Musician:\n",
      "Not sitterry think'd ahler of By\n",
      "Fellows and rage me restill had stand. Henry marry\n",
      "person; of your grey mean cause your rappeals,\n",
      "That say you the word the much persar'd;\n",
      "And yet the king its and thy lather and trone?\n",
      "Therefore, I persuade have face.\n",
      "\n",
      "MENIUS:\n",
      "My lord.\n",
      "\n",
      "Messenger:\n",
      "He's any hath sinst playder I care goodxpleasure,\n",
      "And less not light their \n",
      "---------------\n",
      "\n",
      "ISCALEO:\n",
      "Servaloise thou hast thy brothere.\n",
      "\n",
      "Second Murderer:\n",
      "It in your hearths erefore tellow me.\n",
      "\n",
      "POMPEY:\n",
      "They beseech then must think end the will night thee.\n",
      "\n",
      "HASTINGS:\n",
      "I my groan, poor I should thy sword\n",
      "More poor be tappear'd funcles do service, it\n",
      "ell the sacrament been strenged there; but now, think\n",
      "the is on of his captules head stoward esurpring\n",
      "That think thou diss the to death.\n",
      "\n",
      "BRUTK:\n",
      "They should by the stand a sick: he\n",
      "Will be much partive which fear, and me all meant;\n",
      "So be thy p\n",
      "---------------\n",
      "\n",
      "So many corrected of the nighter Capus best and my very\n",
      "And knew-night wovery sense, and find soon,\n",
      "Of Romeo, base starself.\n",
      "\n",
      "BRUTUS:\n",
      "Then I shall starve leave of their mother,\n",
      "Thou shalt itter cry the flouse of my sound,\n",
      "The woman orrow their lights?\n",
      "\n",
      "Second Senator:\n",
      "Welcome, if your sin heave been treasure,\n",
      "To first, who do pursuised forge, good nor my blood\n",
      "Evething siting how rath-for the good from thy\n",
      "should I should the pite spiritie hated.\n",
      "What we upon you thrumphanks sued away!--and was\n",
      "\n",
      "---------------\n",
      "\n",
      "Hearth on of rathrona:\n",
      "I shall my hang fois, I way the was ea.\n",
      "\n",
      "GLOUCESTER:\n",
      "The gly is my move? O,\n",
      "\n",
      "Senators cons the is heavent:\n",
      "If the the sethink the know not such the shall\n",
      "the red victures the great thou werthink an after\n",
      "to meaning it.\n",
      "\n",
      "ROMEO:\n",
      "The senate like.\n",
      "\n",
      "LEONTES:\n",
      "Come, to fy the unswer villate my to place.\n",
      "\n",
      "POMPEY:\n",
      "The made despassembling too?\n",
      "Do shall said merbow and with speaking the grace:\n",
      "For souls have thing the not poor teven.\n",
      "\n",
      "GLOUCESTER:\n",
      "I that have gold, lord, I did the Cre\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# sample from a trained model to generate new text\n",
    "# (borrowed from nanoGPT, https://github.com/karpathy/nanoGPT) \n",
    "\n",
    "import torch\n",
    "\n",
    "start = \"\\n\" \n",
    "num_samples = 10 # number of samples to draw\n",
    "max_new_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = None #200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "torch.cuda.manual_seed(1337)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# encode the beginning of the prompt\n",
    "if start.startswith('FILE:'):\n",
    "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "        start = f.read()\n",
    "start_ids = encode(start)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "for k in range(num_samples):\n",
    "  y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "  print(decode(y[0].tolist()))\n",
    "  print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxIQtN8bW28u"
   },
   "source": [
    "## **Exercises**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "os4r4-2IW34K"
   },
   "source": [
    "### **Problem 7.1:**\n",
    "\n",
    "Use *pytorch* to implement the simple recurrent neural network  (RNN) structure on Figure 8.25 on page 170, and then extend it to multi-layer RNN models to conduct char-level language modelling on the *tiny-shakespeare* corpus. At last, sample the trained RNN model to generate new text. Compare the RNN model with the transformer model in Example 7.2 in terms of training speed and the quality of generated text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0m5KV8_XL7a"
   },
   "source": [
    "### **Problem 7.2:**\n",
    "\n",
    "Based on the result of Q8.9 (part b) on page 202, use *pytorch* to explicitly implement the backward pass for the attention model in Example 7.1. Make sure the computed gradients are equal to those obtained from the autograd method in Example 7.1. Also compare the self-implemented backward pass with the *pytorch's* autograd method in terms of execution time when running on CPUs or GPUs.  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
