{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab 7: Transformers**\n",
        "\n",
        "---\n",
        "NOTE: This is a lab project accompanying the following book [MLF] and it should be used together with the book.\n",
        "\n",
        "[MLF] *H. Jiang*, \"[Machine Learning Fundamentals: A Concise Introduction](http://wiki.eecs.yorku.ca/user/hj/research:mlfbook)\", Cambridge University Press, 2021.  ([bibtex](http://www.cse.yorku.ca/~hj/mlf-jiang.bib))\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "72pszDXYGC7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this lab is to study the popular transformer achitecture for a simple language modeling task. First, we show how to implement the multi-head attention module in transformers using *pytorch*. Next, we extend it to a multi-layer transformer structure for general sequence modeling purpose. In particular, we use the multi-layer transformer model to perform char-level language modeling on a small text corpus, i.e. the *tiny-shakespeare* corpus. At last, after the transformer is trained, we how that it can be used to generate new text similar to the training samples.\n",
        "\n",
        "Prerequisites: basic understanding on *pytorch*."
      ],
      "metadata": {
        "id": "zWg6ZuV9GVR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **I. Multi-head Self-Attention Module**"
      ],
      "metadata": {
        "id": "uuxeXFn3JqZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Example 7.1:**\n",
        "\n",
        "*Use pytorch to implement a multi-head causal self-attention model similar to Figure 8.27 on page 173. The causal attention means that each output vector depends on only all preceding input vectors but not any vectors appearing after it.  Compare the running speeds of its forward and backward passes when running on CPUs and GPUs.*"
      ],
      "metadata": {
        "id": "6znEB6VJJ16c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All hyper-parameters: (usually we have $d= n \\times h$)\n",
        "*  $d$: embedding dimension\n",
        "*  $B$: batch size\n",
        "*  $T$: block size\n",
        "*  $n$: head number\n",
        "*  $h$: head size\n",
        "\n",
        "\n",
        "1. Given a mini-batch of $B$ input sequences, packed as $\\mathbf{X} \\in \\mathbb{R}^{B \\times d \\times T}$, and a multi-head transformer consisting of all parameter matrices as $\\mathbf{A}^{(j)}, \\mathbf{B}^{(j)}, \\mathbf{C}^{(j)} \\in \\mathbb{R}^{h \\times d}$   ($j=1,2,\\cdots,n$). We first arrange these matrices into three larger parameter matrices,\n",
        "$\\mathbf{A}, \\mathbf{B}, \\mathbf{C} \\in \\mathbb{R}^{nh \\times d}$, and\n",
        "then further pack these three matrices into a single large matrix  $\\mathbf{M} \\in \\mathbb{R}^{3nh \\times d}$.\n",
        "We can generate query, key and value matrices as follows:\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "\\mathbf{Q} \\\\\n",
        "\\mathbf{K} \\\\\n",
        "\\mathbf{V} \\end{bmatrix}  =\n",
        "\\begin{bmatrix}\\mathbf{A}\\\\\n",
        " \\mathbf{B} \\\\\\mathbf{C}\n",
        "\\end{bmatrix}  \\mathbf{X}\n",
        "= \\mathbf{M} \\, \\mathbf{X}\n",
        "$$\n",
        "where $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\in \\mathbb{R}^{B \\times nh \\times T}$.\n",
        "\n",
        "2.   Re-shape the query, key and value matrices as follows:\n",
        "$$\n",
        "\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}: \\mathbb{R}^{B \\times nh \\times T}  → \\mathbb{R}^{B \\times n \\times h \\times T}\n",
        "$$\n",
        "\n",
        "3. Apply the point-wise attention as:\n",
        "$$\n",
        "\\mathbf{Q}^\\intercal \\mathbf{K} \\in \\mathbb{R}^{B \\times n \\times T \\times T}\n",
        "$$\n",
        "If necessary, apply an **upper** triangular masking for causal attetion.\n",
        "\n",
        "4. Apply softmax **column by column**\n",
        "$$\n",
        "\\mathcal{A} =  \\mathrm{softmax} \\Big(\\mathbf{Q}^\\intercal \\mathbf{K} \\Big)\n",
        "\\;\\;\\; (\\in \\mathbb{R}^{B \\times n \\times T \\times T})\n",
        "$$\n",
        "\n",
        "\n",
        "5. Generate the output\n",
        "$$\n",
        "\\mathbf{Z} = \\mathbf{V} \\,  \\mathcal{A}   \\;\\;\\; (\\in \\mathbb{R}^{B \\times n \\times h \\times T})\n",
        "$$\n"
      ],
      "metadata": {
        "id": "5QykZSvtLDnn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vrS05MUpF4sk"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class config():\n",
        "  def __init__(self, batch_size = 10, n_embd = 768, block_size = 128, n_head = 8, causal = True, device='cuda'):\n",
        "    assert n_embd %  n_head == 0\n",
        "    self.batch_size = batch_size\n",
        "    self.n_embd = n_embd\n",
        "    self.block_size = block_size\n",
        "    self.n_head = n_head\n",
        "    self.causal = causal\n",
        "    self.device = device\n",
        "\n",
        "class SelfAttentionModule(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.batch_size = cfg.batch_size\n",
        "        self.n_embd = cfg.n_embd\n",
        "        self.block_size = cfg.block_size\n",
        "        self.n_head = cfg.n_head\n",
        "        self.device = cfg.device\n",
        "        self.causual = cfg.causal\n",
        "        mask = torch.tril(torch.ones(self.block_size, self.block_size)).transpose(0,1).view(1, 1, self.block_size, self.block_size)\n",
        "        mask = mask.to(self.device)\n",
        "        self.register_buffer(\"mask\", mask)\n",
        "\n",
        "    def forward(self, X, M):\n",
        "        B, d, T = X.size() # batch size,  embedding dimensionality (n_embd), block size\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        Q, K, V  = (M @ X).split(d, dim=1)\n",
        "        K = K.view(B, self.n_head, d // self.n_head, T)   # (B, nh, hs, T)\n",
        "        Q = Q.view(B, self.n_head, d // self.n_head, T)   # (B, nh, hs, T)\n",
        "        V = V.view(B, self.n_head, d // self.n_head, T)   # (B, nh, hs, T)\n",
        "\n",
        "        att = (Q.transpose(-2, -1) @ K) * (1.0 / math.sqrt(Q.size(-2)))\n",
        "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-2)   # column-wise softmax\n",
        "        Z = V @ att   # (B, nh, hs, T) x (B, nh, T, T) -> (B, nh, hs, T)\n",
        "\n",
        "        Z = Z.contiguous().view(B, d, T)  # re-assemble all head outputs side by side\n",
        "\n",
        "        return Z\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cfg=config()\n",
        "\n",
        "def loss(model, x, M):\n",
        "  y = model(x,M)\n",
        "  return torch.sum(y*y)\n",
        "\n",
        "X = torch.rand((cfg.batch_size, cfg.n_embd, cfg.block_size), requires_grad=True)\n",
        "M = (torch.rand((3*cfg.n_embd, cfg.n_embd),requires_grad=True) - 0.5) * (1.0/math.sqrt(cfg.n_embd))\n",
        "\n",
        "#######################\n",
        "cfg.device ='cpu'\n",
        "\n",
        "print(f'batch_size={cfg.batch_size} n_embd={cfg.n_embd} block_size={cfg.block_size} n_head={cfg.n_head} device={cfg.device}')\n",
        "\n",
        "X = X.to(cfg.device)\n",
        "M = M.to(cfg.device)\n",
        "\n",
        "mm1=SelfAttentionModule(cfg)\n",
        "\n",
        "ls = loss(mm1, X, M)\n",
        "print(f'pytorch forward pass loss: {ls}')\n",
        "\n",
        "print(f'forward() pass and pytorch auto-grad backward() running on {cfg.device}:')\n",
        "%timeit loss(mm1, X, M)\n",
        "%timeit ls.backward(retain_graph=True)\n",
        "\n",
        "#######################\n",
        "cfg.device ='cuda'\n",
        "\n",
        "print(f'batch_size={cfg.batch_size} n_embd={cfg.n_embd} block_size={cfg.block_size} n_head={cfg.n_head} device={cfg.device}')\n",
        "\n",
        "X = X.to(cfg.device)\n",
        "M = M.to(cfg.device)\n",
        "\n",
        "mm2=SelfAttentionModule(cfg)\n",
        "\n",
        "ls = loss(mm2, X, M)\n",
        "print(f'pytorch forward pass loss: {ls}')\n",
        "\n",
        "print(f'forward() pass and pytorch auto-grad backward() running on {cfg.device}:')\n",
        "%timeit loss(mm2, X, M)\n",
        "%timeit ls.backward(retain_graph=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHNFuyqONLVK",
        "outputId": "ba0b5a61-252b-4e03-ab6a-89700a7021fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_size=10 n_embd=768 block_size=128 n_head=8 device=cpu\n",
            "pytorch forward pass loss: 20090.15234375\n",
            "forward() pass and pytorch auto-grad backward() running on cpu:\n",
            "67.3 ms ± 11.3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
            "108 ms ± 2.3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
            "batch_size=10 n_embd=768 block_size=128 n_head=8 device=cuda\n",
            "pytorch forward pass loss: 20090.15234375\n",
            "forward() pass and pytorch auto-grad backward() running on cuda:\n",
            "2.01 ms ± 11.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
            "10.2 ms ± 1.81 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **II. Transformers for Language Modelling**"
      ],
      "metadata": {
        "id": "YdB6QYWsREFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Example 7.2:**\n",
        "\n",
        "*Use pytorch to implement a multi-layer causal transformer model on page 174, similar to the popular GPT architecture, for char-level language modelling on the tiny-shakespeare corpus.\n",
        "In other words, each char in text is treated as a distinct token in the language model. The transformer is trained to predict next character based on all preceding characters in the transformer input block. After it is trained, use it to generate new text sequences.*"
      ],
      "metadata": {
        "id": "1d7JNtngRUt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download tiny shakespeare text corpus from Google drive\n",
        "\n",
        "!gdown --folder https://drive.google.com/drive/folders/1VtY71Iym2uOC4bUUukJ9I-b1JgR5e18o 2> /dev/null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Es3_2TwNRTxo",
        "outputId": "fd8c111a-5271-4909-9f88-06fa00843e07"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 1Sl0000nPuCW6RcF3b3SL2n9EPJpI582q input.txt\n",
            "Building directory structure completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load text file as a character string; build char-level vocabulary, encoder and decoder\n",
        "#  (borrowed from nanoGPT, https://github.com/karpathy/nanoGPT)\n",
        "\n",
        "train_txt_file = 'tinyshakespeare/input.txt'\n",
        "\n",
        "with open(train_txt_file, 'r') as f:\n",
        "    data = f.read()\n",
        "print(f\"length of dataset in characters: {len(data):,}\")\n",
        "\n",
        "# get all the unique characters that occur in this text\n",
        "chars = sorted(list(set(data)))\n",
        "vocab_size = len(chars)\n",
        "print(\"all the unique characters:\", ''.join(chars))\n",
        "print(f\"vocab size: {vocab_size:,}\")\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "def encode(s):\n",
        "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "def decode(l):\n",
        "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# create the train and test splits\n",
        "n = len(data)\n",
        "train_data = data[:int(n*0.9)]\n",
        "val_data = data[int(n*0.9):]\n",
        "\n",
        "# encode both to integers\n",
        "train_ids = encode(train_data)\n",
        "val_ids = encode(val_data)\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "print(f\"val has {len(val_ids):,} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31WtnUCvUAWr",
        "outputId": "24480f38-834e-40f0-9889-3b258badcfc3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define all parts in a GPT model, such as attention, MLP, LN modules\n",
        "#  (adapted from nanoGPT, https://github.com/karpathy/nanoGPT)\n",
        "\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class config():\n",
        "  def __init__(self, batch_size=10, n_layer=12, n_head=12, n_embd=768, block_size=1024, vocab_size=50304, causal=True, device='cpu'):\n",
        "    assert n_embd %  n_head == 0\n",
        "    self.batch_size = batch_size\n",
        "    self.n_embd = n_embd\n",
        "    self.block_size = block_size\n",
        "    self.n_head = n_head\n",
        "    self.causal = causal\n",
        "    self.device = device\n",
        "    self.n_layer = n_layer\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
        "\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size)).transpose(0,1)\n",
        "                            .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, C // self.n_head, self.n_head).transpose(1, 3) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, C // self.n_head, self.n_head).transpose(1, 3) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, C // self.n_head, self.n_head).transpose(1, 3) # (B, nh, T, hs)\n",
        "\n",
        "        att = (q.transpose(-2, -1) @ k) * (1.0 / math.sqrt(q.size(-2)))\n",
        "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-2)\n",
        "        y = v @ att  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 3).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm([config.n_embd])\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm([config.n_embd])\n",
        "\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm([config.n_embd]),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        # with weight tying when using torch.compile() some warnings get generated:\n",
        "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        # Return the number of parameters in the model.\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ],
      "metadata": {
        "id": "Hn3gM5Z-Umh_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training script to learn GPT models based on all given hyper-parameters\n",
        "#  (adapted from nanoGPT, https://github.com/karpathy/nanoGPT)\n",
        "\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# default config values\n",
        "# I/O\n",
        "\n",
        "######### model size ##########\n",
        "n_layer = 3      # num of layers\n",
        "n_head = 8       # num of attn heads per layer\n",
        "head_size = 96   # size of each attn head\n",
        "###############################\n",
        "\n",
        "###### training hyperparameters ######\n",
        "learning_rate = 6e-4   # max learning rate\n",
        "max_iters = 10000      # total number of training iterations\n",
        "batch_size = 12        # mini-batch size\n",
        "block_size = 128       # block size\n",
        "#####################################\n",
        "\n",
        "# adamw optimizer\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "\n",
        "# learning rate decay settings\n",
        "decay_lr = True # whether to decay the learning rate\n",
        "warmup_iters = 2000 # how many steps to warm up for\n",
        "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
        "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
        "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
        "eval_interval = 1000\n",
        "eval_iters = 200\n",
        "\n",
        "# system\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'bfloat16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "n_embd = n_head * head_size\n",
        "\n",
        "data_tr  = np.array(train_ids, dtype=np.uint16)\n",
        "data_val = np.array(val_ids, dtype=np.uint16)\n",
        "\n",
        "# poor man's data loader\n",
        "def get_batch(split):\n",
        "    data = data_tr if split == 'train' else data_val\n",
        "\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# init a new model from scratch\n",
        "print(\"Initializing a new model from scratch\")\n",
        "conf = config(batch_size=batch_size, n_layer=n_layer, n_head=n_head, n_embd=n_embd,\\\n",
        "              block_size=block_size, vocab_size=vocab_size, device=device)\n",
        "model = GPT(conf)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(beta1, beta2))\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "\n",
        "# training loop\n",
        "X, Y = get_batch('train') # fetch the very first batch\n",
        "t0 = time.time()\n",
        "iter_num = 0\n",
        "\n",
        "while True:\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "\n",
        "    logits, loss = model(X, Y)\n",
        "    X, Y = get_batch('train')\n",
        "\n",
        "    # backward pass, with gradient scaling if training in fp16\n",
        "    scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0:\n",
        "      # timing and logging\n",
        "      t1 = time.time()\n",
        "      dt = t1 - t0\n",
        "      t0 = t1\n",
        "\n",
        "      losses = estimate_loss()\n",
        "      print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f} (time lapses {dt:.4f} seconds)\")\n",
        "\n",
        "    iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71YeBTSgVTyS",
        "outputId": "c8d42115-a3a8-497d-c65a-6bf648a3aa7b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing a new model from scratch\n",
            "number of parameters: 21.29M\n",
            "step 0: train loss 6.0489, val loss 6.1314 (time lapses 1.3229 seconds)\n",
            "step 1000: train loss 1.9487, val loss 2.0776 (time lapses 71.9827 seconds)\n",
            "step 2000: train loss 1.7173, val loss 1.8682 (time lapses 72.5995 seconds)\n",
            "step 3000: train loss 1.6054, val loss 1.7837 (time lapses 72.4764 seconds)\n",
            "step 4000: train loss 1.5453, val loss 1.7346 (time lapses 72.9500 seconds)\n",
            "step 5000: train loss 1.5098, val loss 1.7246 (time lapses 72.6370 seconds)\n",
            "step 6000: train loss 1.4861, val loss 1.6935 (time lapses 72.3111 seconds)\n",
            "step 7000: train loss 1.4586, val loss 1.6863 (time lapses 72.0951 seconds)\n",
            "step 8000: train loss 1.4294, val loss 1.6498 (time lapses 71.9847 seconds)\n",
            "step 9000: train loss 1.4137, val loss 1.6569 (time lapses 71.8970 seconds)\n",
            "step 10000: train loss 1.4048, val loss 1.6409 (time lapses 71.9453 seconds)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample from a trained model to generate new text\n",
        "# (borrowed from nanoGPT, https://github.com/karpathy/nanoGPT)\n",
        "\n",
        "import torch\n",
        "\n",
        "start = \"\\n\"     # specify a text string as prompt\n",
        "#start = \"FILE:tinyQA/prompts.txt\"  #can also specify a file of multiple prompts, use as: \"FILE:prompt.txt\"\n",
        "\n",
        "num_samples = 5 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.9 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = None #200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "\n",
        "seed = 1337\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "# sampling the model based on one prompt\n",
        "def SampleOnePrompt(prompt):\n",
        "    start_ids = encode(prompt)\n",
        "    x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "    for k in range(num_samples):\n",
        "        y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "        print(decode(y[0].tolist()))\n",
        "        print('---------------')\n",
        "\n",
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):     # for multiple prompts\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        Lines = f.readlines()\n",
        "        for prompt in Lines:\n",
        "            SampleOnePrompt(prompt[:-1])  # chop the trailing newline char\n",
        "else:\n",
        "     SampleOnePrompt(start)     # for one prompt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aj-ZBh-cWM15",
        "outputId": "9329026f-c6f5-460c-e8e9-471fbda8ccd4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "ANGELO:\n",
            "And comfort is not the aires boast were?\n",
            "Broach me?\n",
            "I am such by me uuth, her bardshirts,\n",
            "Butwick my faction of one heavy sprinfully come my fade\n",
            "Whomes it ender content Hermions to and quiet.\n",
            "\n",
            "DUKE VINCERDONE:\n",
            "For come humb onceive: therefore hew young Bolingbroke?\n",
            "\n",
            "AUTIO:\n",
            "'Forld, demate kneel, not what evily, the most\n",
            "His haply deeds poor gold; the execute,\n",
            "Turk for treamiss mufficing allowers,\n",
            "Maderive my of tranking, as to discraffair the Earldom.\n",
            "Go could all not rel my hand for hi\n",
            "---------------\n",
            "\n",
            "Men pay to Rome.\n",
            "\n",
            "YORK:\n",
            "Why dead so you myself ancy;\n",
            "But which they should I have not us. I have my sighs:\n",
            "I should so, my many more alive,\n",
            "Or hued the heavy life mouth comfort coureen;\n",
            "How what most it my Rome, sir, come,\n",
            "But sunnish'd, let he need out\n",
            "to accusature but from the way soul of burldness\n",
            "Whose lack'd this herce advise,\n",
            "To I stand plaven in the eate,\n",
            "Though off, if out orning tears more portance whoset,\n",
            "To the silver were belived so him the word\n",
            "By the times often their; you contone\n",
            "---------------\n",
            "\n",
            "Most and breath. Their fellows are. She, the is, do did\n",
            "Till of anon thy hoose on thee, master to sea;\n",
            "Be wereing think of his victor wouldst,\n",
            "And friends abstime Inname a canity to make the and\n",
            "inkery dolement tell the leing buwield to-night\n",
            "each I did the deter your Honest too land;\n",
            "As there cold consume an the vow\n",
            "Shrough he it heart some not is conditing torment:\n",
            "Slad my lord, as belive\n",
            "To cave but forth this name and who hear the tewn,\n",
            "To may mine enemy treless fins yet of Rome,\n",
            "Than art el\n",
            "---------------\n",
            "\n",
            "The smools kills; and place, say, as I\n",
            "Is but of a-father my sun.\n",
            "\n",
            "POMPEY:\n",
            "No, sir,\n",
            "Sir, sir, I well accured my heart he know.\n",
            "\n",
            "SAMPSON:\n",
            "If your son mistrumer trial ruin good afe, cousinhim,\n",
            "And as hour surethere have black'd end,\n",
            "My fafter good madam, Edward's cold, office likeous,\n",
            "And away which wither follow'd our true-wrong,\n",
            "From o'll like punish our own arectator's conver.\n",
            "\n",
            "GLOUCESTER:\n",
            "My lord, which ans my and toward's lanch is he parlet.\n",
            "\n",
            "DUKE VINCERDIO:\n",
            "What mayou must forth not know'd;\n",
            "\n",
            "---------------\n",
            "\n",
            "That latten who cond the tears o' their lives.\n",
            "Grace not me, so the dead me to thee.\n",
            "\n",
            "EDBURY:\n",
            "I love him took a writt of love of this enough,\n",
            "I have so, bold, my with milk your graves in than ymild,\n",
            "and is confound the hollow-there countrymand\n",
            "And bold obsequest allent upon'd:\n",
            "As I watereford my name,\n",
            "Our meanishallow made to our his dead.\n",
            "\n",
            "Clown:\n",
            "Let's hone mad my face, good of the gentler of pared to be\n",
            "her one should moving bodies. I would I tree,\n",
            "And rehem to sleep not teems hronefull man of\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercises**"
      ],
      "metadata": {
        "id": "NxIQtN8bW28u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem 7.1:**\n",
        "\n",
        "Use *pytorch* to implement the simple recurrent neural network  (RNN) structure on Figure 8.25 on page 170, and then extend it to multi-layer RNN models to conduct char-level language modelling on the *tiny-shakespeare* corpus. At last, sample the trained RNN model to generate new text. Compare the RNN model with the transformer model in Example 7.2 in terms of training speed and the quality of generated text."
      ],
      "metadata": {
        "id": "os4r4-2IW34K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Problem 7.2:**\n",
        "\n",
        "Based on the result of Q8.9 (part b) on page 202, use *pytorch* to explicitly implement the backward pass for the attention model in Example 7.1. Make sure the computed gradients are equal to those obtained from the autograd method in Example 7.1. Also compare the self-implemented backward pass with the *pytorch's* autograd method in terms of execution time when running on CPUs or GPUs.  "
      ],
      "metadata": {
        "id": "v0m5KV8_XL7a"
      }
    }
  ]
}